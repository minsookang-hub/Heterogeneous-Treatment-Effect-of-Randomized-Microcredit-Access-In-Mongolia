{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33b43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a397f795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] rows=2,109, cols=2416\n",
      "[load] sample columns: ['case_id', 'hhid', 'rescode.x', 'ind', 'aimag.x', 'soum.x', 'treatment.x', 'followup', 'xxxxxbasichhcharacteristicsxxxxx', 'male', 'rship', 'age.x', '...']\n",
      "[restrict] kept endline via 'followup': n=961\n",
      "[choose] endline outcome Y: 'totalc'\n",
      "[choose] baseline outcome Y0: 'totalc0'\n",
      "[transform] Applied log1p to outcome 'totalc' and baseline 'totalc0'\n",
      "[choose] treatment indicator: 'treatment.x'\n",
      "[treat] column='treatment.x', counts:\n",
      "treatment.x\n",
      "0    260\n",
      "1    350\n",
      "2    351\n",
      "[treat] share any=0.729, indiv=0.364, group=0.365\n",
      "[features] baseline-like by pattern/.x: 896; +always-keep; -ids → 896\n",
      "[features] sample baseline candidates: ['age.x', 'aimag.x', 'alcohol1.x', 'alcohol10', 'alcohol2.x', 'alcohol20', 'alcohol2_m0', 'alcohol3.x', 'alcohol30', 'alcohol3_m0', 'alcohol4.x', 'alcohol40', 'alcohol4_m0', 'alcohol5.x', 'alcohol50', 'alcohol6.x', 'alcohol60', 'alcohol_c0', 'alcohol_ck10', 'alcohol_ck20', '...']\n",
      "[types] numeric-like=877, non-numeric-like=19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1218/1568368608.py:132: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_full[\"treat_any\"]   = treat_any.astype(\"float\")      # cast to float to avoid Int64 NA quirks later\n",
      "/tmp/ipykernel_1218/1568368608.py:133: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_full[\"treat_indiv\"] = treat_indiv.astype(\"float\")\n",
      "/tmp/ipykernel_1218/1568368608.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_full[\"treat_group\"] = treat_group.astype(\"float\")\n",
      "/tmp/ipykernel_1218/1568368608.py:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  X_full[\"baselineY\"]   = baselineY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rows] kept rows with non-missing Y / baselineY / treatment: 961 (dropped 0)\n",
      "[drop] >95% missing: dropping 133 cols\n",
      "[logX] Applied log1p to 308 numeric features.\n",
      "         e.g., ['age.x', 'alcohol4.x', 'alcohol40', 'alcohol4_m0', 'alcohol_c0', 'alcohol_cm0', 'alcohol_pp0', 'alcohol_ps0', 'animals0', 'assets_all0', 'bod0', 'books12.x', '...']\n",
      "[drop] zero-variance columns: 51\n",
      "[scale] standardized 975 numeric features (z-score).\n",
      "[drop] high-cardinality categorical (>100 dummies): ['inigiv', 'inirec'] → dropping 256 cols\n",
      "[shape] final X_proc: n=961, p=719\n",
      "Data shape (n, p): 961 975\n",
      "First few columns: ['age.x', 'aimag.x', 'alcohol1.x', 'alcohol10', 'alcohol2.x', 'alcohol20', 'alcohol2_m0', 'alcohol3.x', 'alcohol30', 'alcohol3_m0', 'alcohol4.x', 'alcohol40', 'alcohol4_m0', 'alcohol5.x', 'alcohol50', 'alcohol_c0', 'alcohol_ck10', 'alcohol_ck20', 'alcohol_ckm0', 'alcohol_cm0', '...']\n",
      "Confirm treatment columns present: True\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Point-Zero Rebuild: Data Loading & Clean Feature Matrix (Week 2 reset, improved)\n",
    "# ================================\n",
    "import pandas as pd, numpy as np, re\n",
    "from collections import Counter\n",
    "\n",
    "# ---- Helper: small pretty list\n",
    "def headlist(x, k=12): \n",
    "    return list(x)[:k] + ([\"...\"] if len(x) > k else [])\n",
    "\n",
    "# ---- 0) Load & normalize column names\n",
    "Merged = pd.read_csv(\"merged_full.csv\", low_memory=False)\n",
    "Merged.columns = [str(c).strip().lower() for c in Merged.columns]\n",
    "\n",
    "print(f\"[load] rows={len(Merged):,}, cols={len(Merged.columns)}\")\n",
    "print(\"[load] sample columns:\", headlist(Merged.columns))\n",
    "\n",
    "# ---- 1) Restrict to endline/follow-up if present\n",
    "endline_filters = [\n",
    "    (\"followup\", lambda s: s == 1),\n",
    "    (\"endline\",  lambda s: s == 1),\n",
    "    (\"post\",     lambda s: s == 1),\n",
    "    (\"wave\",     lambda s: s.astype(str).str.lower().isin([\"1\",\"end\",\"endline\",\"post\"])),\n",
    "]\n",
    "AppliedFilter = None\n",
    "for col, cond in endline_filters:\n",
    "    if col in Merged.columns:\n",
    "        try:\n",
    "            mask = cond(Merged[col])\n",
    "            if mask.sum() > 0 and mask.sum() < len(Merged):\n",
    "                Merged = Merged.loc[mask].copy()\n",
    "                AppliedFilter = col\n",
    "                print(f\"[restrict] kept endline via '{col}': n={len(Merged):,}\")\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "if AppliedFilter is None:\n",
    "    print(\"[restrict] no explicit endline/follow-up flag found or usable; using full sample.\")\n",
    "\n",
    "# ---- 2) Choose outcome Y and baseline outcome Y0 (robust patterns)\n",
    "def pick_first_exist(candidates, where, label):\n",
    "    for c in candidates:\n",
    "        if c in where:\n",
    "            print(f\"[choose] {label}: '{c}'\")\n",
    "            return c\n",
    "    raise KeyError(f\"No candidate found for {label}. Tried: {candidates}\")\n",
    "\n",
    "Y_candidates  = [\"totalc\", \"consumption1\", \"totalc_end\", \"totalc1\", \"cons_total1\", \"consumption_end\"]\n",
    "Y0_candidates = [\"totalc0\", \"consumption0\", \"totalc_base\", \"cons_total0\", \"consumption_baseline\"]\n",
    "\n",
    "Y_col  = pick_first_exist(Y_candidates,  Merged.columns, \"endline outcome Y\")\n",
    "Y0_col = pick_first_exist(Y0_candidates, Merged.columns, \"baseline outcome Y0\")\n",
    "\n",
    "Y_series  = pd.to_numeric(Merged[Y_col],  errors=\"coerce\")\n",
    "baselineY = pd.to_numeric(Merged[Y0_col], errors=\"coerce\")\n",
    "\n",
    "# ---- Optional: log-transform outcome and its baseline counterpart\n",
    "USE_LOG = True  # flip to False if you want to go back to levels\n",
    "if USE_LOG:\n",
    "    Y_series  = np.log1p(Y_series.astype(float))\n",
    "    baselineY = np.log1p(baselineY.astype(float))\n",
    "    print(f\"[transform] Applied log1p to outcome '{Y_col}' and baseline '{Y0_col}'\")\n",
    "else:\n",
    "    print(f\"[transform] Keeping outcome '{Y_col}' and baseline '{Y0_col}' in levels\")\n",
    "\n",
    "# ---- 3) Treatment (village-level assignments with 0/1/2)\n",
    "treat_candidates = [\"treatment.x\", \"treated\", \"treatment\", \"treat\", \"assign\", \"assign_treat\", \"village_treat\"]\n",
    "treat_col = pick_first_exist(treat_candidates, Merged.columns, \"treatment indicator\")\n",
    "\n",
    "# [MOD] Robust parse of tri-valued assignment: 0=control, 1=individual, 2=group\n",
    "treat_raw = pd.to_numeric(Merged[treat_col], errors=\"coerce\")\n",
    "\n",
    "treat_any   = (treat_raw > 0).astype(\"Int64\")         # pooled ITT\n",
    "treat_indiv = (treat_raw == 1).astype(\"Int64\")\n",
    "treat_group = (treat_raw == 2).astype(\"Int64\")\n",
    "\n",
    "# [MOD] Diagnostics\n",
    "vc = treat_raw.value_counts(dropna=False).sort_index()\n",
    "print(f\"[treat] column='{treat_col}', counts:\\n{vc.to_string()}\")\n",
    "print(f\"[treat] share any={float(treat_any.mean(skipna=True)):.3f}, \"\n",
    "      f\"indiv={float(treat_indiv.mean(skipna=True)):.3f}, group={float(treat_group.mean(skipna=True)):.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---- 4) Candidate baseline features (improved rules)\n",
    "def is_baseline_col(c):\n",
    "    return bool(re.search(r\"(?:^.*0$|_0$|base|baseline|^pre_)\", c))\n",
    "\n",
    "cols = set(Merged.columns)\n",
    "paired_y = {c for c in cols if c.endswith(\".y\") and c[:-2]+\".x\" in cols}\n",
    "baseline_like = [c for c in cols if (is_baseline_col(c) or c.endswith(\".x\")) and c not in paired_y]\n",
    "\n",
    "# Always-keep core baseline controls (demographics & geography) even if they don't match baseline regex\n",
    "ALWAYS_KEEP = [\n",
    "    \"male\", \"female\", \"age\", \"age.x\", \"age_head\", \"age_head0\",\n",
    "    \"rship\", \"relation\", \"relation_to_head\", \"rship.x\",\n",
    "    \"aimag\", \"aimag.x\", \"soum\", \"soum.x\", \"region\", \"region0\", \"province\", \"province0\",\n",
    "    \"urban\", \"urban0\",\n",
    "    \"hhsize\", \"hhsize0\", \"children\", \"children0\", \"dependents\", \"dependents0\",\n",
    "    \"educ_head\", \"educ_head0\", \"married\", \"married0\",\n",
    "]\n",
    "\n",
    "# Exclude obvious identifiers regardless (prevent leakage)\n",
    "EXCLUDE_IDS = [\n",
    "    \"case_id\", \"hhid\", \"household_id\", \"person_id\", \"cluster_id\", \"village_id\",\n",
    "    \"rescode\", \"rescode.x\", \"rescode.y\", \"ind\", \"indid\", \"enumerator\", \"survey_id\"\n",
    "]\n",
    "\n",
    "EXCLUDE_EXPLICIT = {Y_col, Y0_col, treat_col, \"baselineY\"}\n",
    "baseline_candidates = sorted(set(baseline_like + [c for c in ALWAYS_KEEP if c in cols])\n",
    "                             - set(EXCLUDE_IDS) - EXCLUDE_EXPLICIT)\n",
    "\n",
    "print(f\"[features] baseline-like by pattern/.x: {len(baseline_like)}; +always-keep; -ids → {len(baseline_candidates)}\")\n",
    "print(\"[features] sample baseline candidates:\", headlist(baseline_candidates, 20))\n",
    "\n",
    "# ---- 5) Assemble X_full (baseline-only set) + add treatment and baselineY\n",
    "X_full = Merged[baseline_candidates].copy()\n",
    "\n",
    "# Identify numeric vs categorical BEFORE coercion (diagnostics)\n",
    "types_before = {c: str(Merged[c].dtype) for c in X_full.columns}\n",
    "num_like_cols = [c for c in X_full.columns if pd.api.types.is_numeric_dtype(Merged[c])]\n",
    "obj_like_cols = [c for c in X_full.columns if not pd.api.types.is_numeric_dtype(Merged[c])]\n",
    "print(f\"[types] numeric-like={len(num_like_cols)}, non-numeric-like={len(obj_like_cols)}\")\n",
    "\n",
    "# Coerce numerics; keep categoricals as strings for one-hot later\n",
    "for c in num_like_cols:\n",
    "    X_full[c] = pd.to_numeric(X_full[c], errors=\"coerce\")\n",
    "for c in obj_like_cols:\n",
    "    X_full[c] = Merged[c].astype(\"string\")\n",
    "\n",
    "# [MOD] Explicit treatment controls (numeric) + baseline outcome\n",
    "X_full[\"treat_any\"]   = treat_any.astype(\"float\")      # cast to float to avoid Int64 NA quirks later\n",
    "X_full[\"treat_indiv\"] = treat_indiv.astype(\"float\")\n",
    "X_full[\"treat_group\"] = treat_group.astype(\"float\")\n",
    "X_full[\"baselineY\"]   = baselineY\n",
    "\n",
    "# ---- 6) Row filter: require Y, baselineY, and non-missing raw treatment\n",
    "keep_rows = (\n",
    "    Y_series.notna() &\n",
    "    X_full[\"baselineY\"].notna() &\n",
    "    treat_raw.notna()\n",
    ")\n",
    "n_before = len(X_full)\n",
    "X_full = X_full.loc[keep_rows].copy()\n",
    "Y_use  = Y_series.loc[keep_rows].copy()\n",
    "print(f\"[rows] kept rows with non-missing Y / baselineY / treatment: {len(X_full):,} (dropped {n_before - len(X_full):,})\")\n",
    "\n",
    "# ---- 7) Drop columns with extreme missingness (>95% NaN)\n",
    "missing_rate = X_full.isna().mean().sort_values(ascending=False)\n",
    "too_missing = list(missing_rate[missing_rate > 0.95].index)\n",
    "\n",
    "# [MOD] Protect treatment and baselineY from being dropped\n",
    "PROTECT = {\"treat_any\",\"treat_indiv\",\"treat_group\",\"baselineY\"}\n",
    "too_missing = [c for c in too_missing if c not in PROTECT]\n",
    "if too_missing:\n",
    "    print(f\"[drop] >95% missing: dropping {len(too_missing)} cols\")\n",
    "    X_full = X_full.drop(columns=too_missing)\n",
    "    \n",
    "    \n",
    "# ---- 8) Impute numeric mean; categorical mode (unchanged)\n",
    "num_cols = [c for c in X_full.columns if pd.api.types.is_numeric_dtype(X_full[c])]\n",
    "cat_cols = [c for c in X_full.columns if c not in num_cols]\n",
    "\n",
    "if num_cols:\n",
    "    X_full[num_cols] = X_full[num_cols].apply(lambda col: col.fillna(col.mean()))\n",
    "if cat_cols:\n",
    "    for c in cat_cols:\n",
    "        mode = X_full[c].mode(dropna=True)\n",
    "        fill = mode.iloc[0] if not mode.empty else \"missing\"\n",
    "        X_full[c] = X_full[c].fillna(fill)\n",
    "\n",
    "# ---- 8.5) Log-transform eligible numeric X-features (safe log1p)\n",
    "LOG_PROTECT = {\"treat_any\", \"treat_indiv\", \"treat_group\", \"baselineY\"}  # never log these\n",
    "\n",
    "def looks_logged(name: str) -> bool:\n",
    "    return bool(re.search(r\"(?:^log[_]?|[_](log|ln)$|[_]log[_]?|[_]ln$|^ln[_]?)\", name))\n",
    "\n",
    "def is_indicator_like(s: pd.Series) -> bool:\n",
    "    # Heuristic: very few unique values → likely categorical/indicator\n",
    "    return s.nunique(dropna=True) <= 10 and set(s.dropna().unique()).issubset({0, 1, 2, 3})\n",
    "\n",
    "def log_candidate(col: str, s: pd.Series) -> bool:\n",
    "    if col in LOG_PROTECT:\n",
    "        return False\n",
    "    if looks_logged(col):\n",
    "        return False\n",
    "    if is_indicator_like(s):\n",
    "        return False\n",
    "    # need non-negative values for log1p\n",
    "    if s.min() < 0:\n",
    "        return False\n",
    "    # avoid logging variables that are already tiny/scaled (e.g., shares)\n",
    "    if s.quantile(0.95) <= 1.0:\n",
    "        return False\n",
    "    # avoid columns with too few distinct values\n",
    "    if s.nunique(dropna=True) < 20:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "log_applied = []\n",
    "for c in [c for c in X_full.columns if pd.api.types.is_numeric_dtype(X_full[c])]:\n",
    "    s = X_full[c]\n",
    "    if log_candidate(c, s):\n",
    "        X_full[c] = np.log1p(s.astype(float))\n",
    "        log_applied.append(c)\n",
    "\n",
    "print(f\"[logX] Applied log1p to {len(log_applied)} numeric features.\")\n",
    "if log_applied:\n",
    "    print(\"         e.g.,\", headlist(sorted(log_applied), 12))\n",
    "\n",
    "        \n",
    "        \n",
    "# ---- 9) One-hot encode categoricals, then drop zero-variance columns\n",
    "X_cat = pd.get_dummies(X_full[cat_cols], drop_first=True) if cat_cols else pd.DataFrame(index=X_full.index)\n",
    "X_num = X_full[num_cols] if num_cols else pd.DataFrame(index=X_full.index)\n",
    "\n",
    "# [MOD] Ensure protected numerics are present\n",
    "for k in [\"treat_any\",\"treat_indiv\",\"treat_group\",\"baselineY\"]:\n",
    "    if k not in X_num.columns and k in X_full.columns:\n",
    "        X_num[k] = X_full[k]\n",
    "\n",
    "X_proc = pd.concat([X_num, X_cat], axis=1)\n",
    "\n",
    "# Zero variance after impute/encode\n",
    "nuniq = X_proc.nunique(dropna=True)\n",
    "zerovar = list(nuniq[nuniq <= 1].index)\n",
    "# [MOD] Don’t drop protected even if degenerate in a subset\n",
    "zerovar = [c for c in zerovar if c not in PROTECT]\n",
    "if zerovar:\n",
    "    print(f\"[drop] zero-variance columns: {len(zerovar)}\")\n",
    "    X_proc = X_proc.drop(columns=zerovar)\n",
    "\n",
    "    \n",
    "    # ---- 9.5) Standardize (z-score) all numeric features for ML comparability\n",
    "# Keep a copy of the scaler means/stds if you want to inverse-transform later\n",
    "num_cols_proc = [c for c in X_proc.columns if pd.api.types.is_numeric_dtype(X_proc[c])]\n",
    "\n",
    "# Guard: don't standardize degenerate columns (already removed above), just in case\n",
    "nondeg = [c for c in num_cols_proc if X_proc[c].std(ddof=0) > 0]\n",
    "\n",
    "X_scaled_num = X_proc[nondeg].apply(lambda col: (col - col.mean()) / col.std(ddof=0))\n",
    "X_scaled = pd.concat([X_scaled_num, X_proc.drop(columns=nondeg)], axis=1)\n",
    "\n",
    "print(f\"[scale] standardized {len(nondeg)} numeric features (z-score).\")\n",
    "\n",
    "\n",
    "    \n",
    "# ---- 10) Drop high-cardinality categorical dummies (likely IDs) as a safety net\n",
    "K = 100\n",
    "if not X_cat.empty:\n",
    "    orig_cat = X_cat.columns.to_series().str.extract(r\"^(.*?)[_=].*$\", expand=False)\n",
    "    dummy_counts = orig_cat.value_counts()\n",
    "    id_like_cats = dummy_counts[dummy_counts > K].index.tolist()\n",
    "    if id_like_cats:\n",
    "        drop_cols = [c for c in X_cat.columns if any(c.startswith(name + \"_\") or c.startswith(name + \"=\") for name in id_like_cats)]\n",
    "        # [MOD] No protected vars live in X_cat, but guard anyway\n",
    "        drop_cols = [c for c in drop_cols if c not in PROTECT]\n",
    "        if drop_cols:\n",
    "            print(f\"[drop] high-cardinality categorical (>{K} dummies): {id_like_cats} → dropping {len(drop_cols)} cols\")\n",
    "            X_proc = X_proc.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "print(f\"[shape] final X_proc: n={X_proc.shape[0]:,}, p={X_proc.shape[1]:,}\")\n",
    "if X_proc.shape[1] == 0:\n",
    "    print(\"[warn] zero features after cleaning. Diagnostics:\")\n",
    "    print(\"  - Top missing columns:\\n\", missing_rate.head(20))\n",
    "    print(\"  - Example dtypes (before):\", Counter(types_before.values()))\n",
    "    print(\"  - Kept cat_cols:\", headlist(cat_cols, 20))\n",
    "    print(\"  - Kept num_cols:\", headlist(num_cols, 20))\n",
    "\n",
    "\n",
    "# ---- 11) Build final dataset for modeling (use scaled features)\n",
    "Data_all = pd.concat([Y_use.rename(Y_col), X_scaled], axis=1)\n",
    "Y = Data_all[Y_col].to_numpy()\n",
    "X = Data_all.drop(columns=[Y_col])\n",
    "\n",
    "print(\"Data shape (n, p):\", X.shape[0], X.shape[1])\n",
    "print(\"First few columns:\", headlist(X.columns, 20))\n",
    "print(\"Confirm treatment columns present:\",\n",
    "      all(k in X.columns for k in [\"treat_any\",\"treat_indiv\",\"treat_group\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812577bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Summary Statistics + Visuals (Project One)\n",
    "# ================================\n",
    "# Requires: merged_full.csv in the CWD\n",
    "# Outputs:\n",
    "#   - table1_baseline.csv / table1_baseline.tex\n",
    "#   - figs/*.png (Histogram, box/violin, scatter+smooth, aimag bars, appendix densities)\n",
    "# Notes:\n",
    "#   - Uses only pandas/numpy/matplotlib; statsmodels LOWESS is optional (guarded).\n",
    "#   - Does NOT change your core dataset; transforms are temp copies.\n",
    "# ================================\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Config ----------\n",
    "INPUT_CSV = \"merged_full.csv\"\n",
    "OUT_DIR   = \"figs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Columns (align with your pipeline / variable naming)\n",
    "# Outcome + treatment\n",
    "Y_end_col   = \"totalc\"            # endline total consumption (will use log)\n",
    "Y_base_col  = \"baselineY\"         # your logged baseline consumption aggregate (already log1p in pipeline)\n",
    "TREAT_GROUP = \"treat_group\"       # 1 = joint-liability offer, 0 = control\n",
    "TREAT_ANY   = \"treat_any\"\n",
    "\n",
    "# Curated baseline X’s (raw names in your dataset)\n",
    "vars_enterprise = [\"enterprise.x\", \"soleent.x\", \"jointent.x\", \"hours_ent.x\", \"hours_wage.x\", \"profit0\"]\n",
    "vars_assets     = [\"assets_all0\", \"animals0\", \"sheep0\", \"goats0\", \"cattle0\", \"horses0\",\n",
    "                   \"dwellingvalue0\", \"ownsdwelling.x\", \"dwellingsize.x\", \"rooms.x\"]\n",
    "vars_income_exp = [\"hhincome0\", \"totalexp0\", \"foodc0\", \"nondurc0\", \"durc0\"]\n",
    "vars_demo       = [\"hhsize\", \"age.x\", \"edulow.x\", \"eduvoc.x\", \"eduhigh.x\", \"male\"]\n",
    "vars_geo        = [\"aimag.x\", \"soum.x\", \"monthsliving0\", \"yearsliving0\", \"stillresid.x\"]\n",
    "vars_transfers  = [\"rec_y0\", \"gave_y0\", \"mobile1.x\", \"tv1.x\"]\n",
    "\n",
    "CURATED_VARS = (\n",
    "    [Y_base_col] +\n",
    "    vars_enterprise +\n",
    "    vars_assets +\n",
    "    vars_income_exp +\n",
    "    vars_demo +\n",
    "    [v for v in vars_geo if v not in [\"aimag.x\",\"soum.x\"]] +  # keep numeric tenure/land; aimag/soum used for groups/figs\n",
    "    vars_transfers\n",
    ")\n",
    "\n",
    "# Variables to log1p for distributional stability (won’t overwrite raw)\n",
    "LOG1P_VARS = [\"assets_all0\", \"dwellingvalue0\", \"hhincome0\", \"totalexp0\",\n",
    "              \"foodc0\", \"nondurc0\", \"durc0\", \"rec_y0\", \"gave_y0\"]\n",
    "\n",
    "# Helper labels for LaTeX table (left column). Edit as needed.\n",
    "LABELS = {\n",
    "    Y_base_col:          \"Baseline log consumption (log)\",\n",
    "    \"enterprise.x\":      \"Any enterprise (0/1)\",\n",
    "    \"soleent.x\":         \"Sole enterprise (0/1)\",\n",
    "    \"jointent.x\":        \"Joint enterprise (0/1)\",\n",
    "    \"hours_ent.x\":       \"Hours in enterprise (baseline)\",\n",
    "    \"hours_wage.x\":      \"Hours in wage work (baseline)\",\n",
    "    \"profit0\":           \"Business profit (baseline)\",\n",
    "    \"assets_all0\":       \"Assets (baseline)\",\n",
    "    \"animals0\":          \"Livestock units (baseline)\",\n",
    "    \"sheep0\":            \"Sheep owned (baseline)\",\n",
    "    \"goats0\":            \"Goats owned (baseline)\",\n",
    "    \"cattle0\":           \"Cattle owned (baseline)\",\n",
    "    \"horses0\":           \"Horses owned (baseline)\",\n",
    "    \"dwellingvalue0\":    \"Dwelling value (baseline)\",\n",
    "    \"ownsdwelling.x\":    \"Owns dwelling (0/1)\",\n",
    "    \"dwellingsize.x\":    \"Dwelling size (m$^2$)\",\n",
    "    \"rooms.x\":           \"Number of rooms\",\n",
    "    \"hhincome0\":         \"Income (baseline)\",\n",
    "    \"totalexp0\":         \"Total expenditure (baseline)\",\n",
    "    \"foodc0\":            \"Food expenditure (baseline)\",\n",
    "    \"nondurc0\":          \"Nondurables (baseline)\",\n",
    "    \"durc0\":             \"Durables (baseline)\",\n",
    "    \"hhsize\":            \"Household size\",\n",
    "    \"age.x\":             \"Avg. age (baseline)\",\n",
    "    \"edulow.x\":          \"Primary or less (0/1)\",\n",
    "    \"eduvoc.x\":          \"Vocational (0/1)\",\n",
    "    \"eduhigh.x\":         \"High school+ (0/1)\",\n",
    "    \"male\":              \"Male head (0/1)\",\n",
    "    \"monthsliving0\":     \"Months in community\",\n",
    "    \"yearsliving0\":      \"Years in community\",\n",
    "    \"stillresid.x\":      \"Still resident (0/1)\",\n",
    "    \"rec_y0\":            \"Transfers received (baseline)\",\n",
    "    \"gave_y0\":           \"Transfers given (baseline)\",\n",
    "    \"mobile1.x\":         \"Owns mobile (0/1)\",\n",
    "    \"tv1.x\":             \"Owns TV (0/1)\",\n",
    "}\n",
    "\n",
    "# ===== Compatibility shim: align with your cleaning pipeline =====\n",
    "import pandas as pd, numpy as np, re, os, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "INPUT_CSV = \"merged_full.csv\"\n",
    "OUT_DIR   = \"figs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 0) Load + lowercase columns (matches your pipeline)\n",
    "df = pd.read_csv(INPUT_CSV, low_memory=False)\n",
    "df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "\n",
    "# 1) Identify endline and baseline outcome columns (like your pick_first_exist)\n",
    "Y_candidates  = [\"totalc\",\"consumption1\",\"totalc_end\",\"totalc1\",\"cons_total1\",\"consumption_end\"]\n",
    "Y0_candidates = [\"totalc0\",\"consumption0\",\"totalc_base\",\"cons_total0\",\"consumption_baseline\"]\n",
    "\n",
    "def pick_first_exist(cands, cols, label):\n",
    "    for c in cands:\n",
    "        if c in cols:\n",
    "            print(f\"[choose] {label}: '{c}'\")\n",
    "            return c\n",
    "    raise KeyError(f\"No candidate found for {label}. Tried: {cands}\")\n",
    "\n",
    "Y_col  = pick_first_exist(Y_candidates,  df.columns, \"endline outcome Y\")\n",
    "Y0_col = pick_first_exist(Y0_candidates, df.columns, \"baseline outcome Y0\")\n",
    "\n",
    "# Create log1p baselineY (to mirror your pipeline)\n",
    "df[\"baseliney\"] = np.log1p(pd.to_numeric(df[Y0_col], errors=\"coerce\"))\n",
    "# Also keep a logged endline outcome for figures if needed later\n",
    "df[\"log_\" + Y_col] = np.log1p(pd.to_numeric(df[Y_col], errors=\"coerce\"))\n",
    "\n",
    "# 2) Recover treatment from tri-valued assignment (0=control, 1=individual, 2=group)\n",
    "treat_candidates = [\"treatment.x\",\"treated\",\"treatment\",\"treat\",\"assign\",\"assign_treat\",\"village_treat\"]\n",
    "treat_col = pick_first_exist(treat_candidates, df.columns, \"treatment indicator\")\n",
    "\n",
    "treat_raw = pd.to_numeric(df[treat_col], errors=\"coerce\")\n",
    "df[\"treat_any\"]   = (treat_raw > 0).astype(\"Int64\")\n",
    "df[\"treat_indiv\"] = (treat_raw == 1).astype(\"Int64\")\n",
    "df[\"treat_group\"] = (treat_raw == 2).astype(\"Int64\")\n",
    "\n",
    "print(f\"[treat] column='{treat_col}', shares any={float(df['treat_any'].mean(skipna=True)):.3f}, \"\n",
    "      f\"indiv={float(df['treat_indiv'].mean(skipna=True)):.3f}, group={float(df['treat_group'].mean(skipna=True)):.3f}\")\n",
    "\n",
    "\n",
    "# ---------- Balance / Summary Table ----------\n",
    "def std_diff(x_t, x_c):\n",
    "    \"\"\"Standardized difference: (mean_t - mean_c) / pooled sd.\"\"\"\n",
    "    mt, mc = np.nanmean(x_t), np.nanmean(x_c)\n",
    "    vt, vc = np.nanvar(x_t, ddof=1), np.nanvar(x_c, ddof=1)\n",
    "    # Pooled SD (two-sample, unweighted by group sizes in denominator as in std-diff convention)\n",
    "    sp = math.sqrt((vt + vc) / 2.0) if np.isfinite(vt) and np.isfinite(vc) else np.nan\n",
    "    return (mt - mc) / sp if sp and sp > 0 else np.nan\n",
    "\n",
    "def summarize_table(df, vars_list, treat_col=TREAT_GROUP, treat_val=1, dropna_axis=0):\n",
    "    # Filter groups\n",
    "    treated = df.loc[df[treat_col] == treat_val]\n",
    "    control = df.loc[df[treat_col] == 0]\n",
    "\n",
    "    rows = []\n",
    "    for v in vars_list:\n",
    "        if v not in df.columns:\n",
    "            continue\n",
    "        x_all = df[v]\n",
    "        x_t   = treated[v]\n",
    "        x_c   = control[v]\n",
    "\n",
    "        mean_all = np.nanmean(x_all)\n",
    "        sd_all   = np.nanstd(x_all, ddof=1)\n",
    "        mean_c   = np.nanmean(x_c)\n",
    "        mean_t   = np.nanmean(x_t)\n",
    "        sdiff    = std_diff(x_t, x_c)\n",
    "\n",
    "        rows.append({\n",
    "            \"variable\": v,\n",
    "            \"label\": LABELS.get(v, v),\n",
    "            \"Mean (All)\": mean_all,\n",
    "            \"SD (All)\": sd_all,\n",
    "            \"Mean (Control)\": mean_c,\n",
    "            \"Mean (JL Offer)\": mean_t,\n",
    "            \"Std. Diff\": sdiff,\n",
    "            \"N (All)\": x_all.notna().sum()\n",
    "        })\n",
    "\n",
    "    tab = pd.DataFrame(rows)\n",
    "    # Order: put baselineY first, then logical groupings\n",
    "    return tab\n",
    "\n",
    "# Build the final list (keep existing columns only)\n",
    "final_vars = [v for v in CURATED_VARS if v in df.columns]\n",
    "\n",
    "\n",
    "# --- after loading CSV and lowercasing columns ---\n",
    "\n",
    "# 1) Fix baselineY naming and config\n",
    "Y_base_col = \"baseliney\"  # keep config in sync with lowercase creation\n",
    "df[\"baseliney\"] = np.log1p(pd.to_numeric(df[Y0_col], errors=\"coerce\"))\n",
    "\n",
    "# 2) Create log1p columns you reference later (assets, etc.)\n",
    "for c in [\"assets_all0\",\"dwellingvalue0\",\"hhincome0\",\"totalexp0\",\"foodc0\",\"nondurc0\",\"durc0\",\"rec_y0\",\"gave_y0\"]:\n",
    "    if c in df.columns:\n",
    "        df[f\"log1p_{c}\"] = np.log1p(pd.to_numeric(df[c], errors=\"coerce\"))\n",
    "\n",
    "# 3) Explicit recodes for supposed 0/1 indicators (defensive)\n",
    "#    Adjust these mappings to your actual codebook if needed.\n",
    "def binarize(series, true_values={1, \"1\", \"yes\", \"y\", True}):\n",
    "    x = pd.Series(series).copy()\n",
    "    # try numeric first\n",
    "    xn = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # if coded as 1/2/3, treat 1 as owner/yes, else 0\n",
    "    out = np.where(xn == 1, 1, np.where(xn == 0, 0, np.nan))\n",
    "    # fallback: strings\n",
    "    mask = pd.isna(out)\n",
    "    out[mask] = np.where(x[mask].astype(str).str.strip().str.lower().isin({str(v).lower() for v in true_values}), 1, 0)\n",
    "    return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "binary_vars = [\"enterprise.x\",\"soleent.x\",\"jointent.x\",\"ownsdwelling.x\",\"edulow.x\",\"eduvoc.x\",\"eduhigh.x\",\"male\",\"stillresid.x\",\"mobile1.x\",\"tv1.x\"]\n",
    "for v in binary_vars:\n",
    "    if v in df.columns:\n",
    "        # only recode if values look non-binary\n",
    "        vals = pd.to_numeric(df[v], errors=\"coerce\")\n",
    "        if not set(pd.unique(vals.dropna())).issubset({0,1}):\n",
    "            df[v] = binarize(df[v])\n",
    "\n",
    "# 4) Baseline/endline enterprise columns: ensure you have both and label correctly\n",
    "#    If your dataset uses enterprise.y for endline, keep baseline in .x and endline in .y\n",
    "if \"enterprise.y\" in df.columns and \"enterprise.x\" in df.columns:\n",
    "    df[\"enterprise_baseline\"] = binarize(df[\"enterprise.x\"])\n",
    "    df[\"enterprise_endline\"]  = binarize(df[\"enterprise.y\"])\n",
    "    # (Optionally) update your CURATED_VARS and LABELS to use these canonical names\n",
    "    LABELS[\"enterprise_baseline\"] = \"Any enterprise at baseline (0/1)\"\n",
    "    LABELS[\"enterprise_endline\"]  = \"Any enterprise at endline (0/1)\"\n",
    "\n",
    "\n",
    "table1 = summarize_table(df, final_vars, treat_col=TREAT_GROUP, treat_val=1)\n",
    "# Save CSV\n",
    "table1.to_csv(\"table1_baseline.csv\", index=False)\n",
    "\n",
    "# Pretty LaTeX (minimal formatting; you can style in Overleaf)\n",
    "def to_latex_table(tab, fname=\"table1_baseline.tex\", caption=\"Baseline Characteristics (Curated)\", label=\"tab:baseline\"):\n",
    "    show = tab.copy()\n",
    "    show = show[[\"label\",\"Mean (All)\",\"SD (All)\",\"Mean (Control)\",\"Mean (JL Offer)\",\"Std. Diff\",\"N (All)\"]]\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[H]\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{\" + caption + \"}\\n\")\n",
    "        f.write(\"\\\\label{\" + label + \"}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lrrrrrr}\\n\\\\toprule\\n\")\n",
    "        f.write(\"Variable & Mean (All) & SD (All) & Mean (Control) & Mean (JL Offer) & Std. Diff & N \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in show.iterrows():\n",
    "            f.write(f\"{r['label']} & {r['Mean (All)']:.3f} & {r['SD (All)']:.3f} & {r['Mean (Control)']:.3f} & {r['Mean (JL Offer)']:.3f} & {r['Std. Diff']:.3f} & {int(r['N (All)'])} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\begin{minipage}{0.92\\\\linewidth}\\\\vspace{0.6ex}\\\\footnotesize\\n\")\n",
    "        f.write(\"Notes: Means and SDs computed using household weights if applicable (not applied here). Standardized differences use pooled SD. Variables with heavy right tails appear log-transformed in figures (log1p). Standard errors in later sections are clustered at the village level.\\n\")\n",
    "        f.write(\"\\\\end{minipage}\\n\")\n",
    "        f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "to_latex_table(table1)\n",
    "\n",
    "print(\"[ok] Wrote table1_baseline.csv and table1_baseline.tex\")\n",
    "\n",
    "# ---------- Figures ----------\n",
    "# Helper: histogram\n",
    "def save_hist(col, bins=30, fname=\"hist.png\", title=None, xlabel=None):\n",
    "    x = df[col].dropna().values\n",
    "    plt.figure(figsize=(6,4.2))\n",
    "    plt.hist(x, bins=bins)\n",
    "    if title: plt.title(title)\n",
    "    if xlabel: plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Helper: violin/box by group\n",
    "def save_violin_by(col, group_col, fname=\"violin.png\", title=None, labels=(\"0\",\"1\")):\n",
    "    g0 = df.loc[df[group_col]==0, col].dropna().values\n",
    "    g1 = df.loc[df[group_col]==1, col].dropna().values\n",
    "    plt.figure(figsize=(6,4.2))\n",
    "    parts = plt.violinplot([g0, g1], showmeans=True, showextrema=False, showmedians=False)\n",
    "    plt.xticks([1,2], labels)\n",
    "    if title: plt.title(title)\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_box_by(col, group_col, fname=\"box.png\", title=None, labels=(\"0\",\"1\")):\n",
    "    g0 = df.loc[df[group_col]==0, col].dropna().values\n",
    "    g1 = df.loc[df[group_col]==1, col].dropna().values\n",
    "    plt.figure(figsize=(6,4.2))\n",
    "    plt.boxplot([g0, g1], showmeans=True)\n",
    "    plt.xticks([1,2], labels)\n",
    "    if title: plt.title(title)\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Helper: scatter + simple smooth (poly or LOWESS if statsmodels available)\n",
    "def save_scatter_smooth(xcol, ycol, fname=\"scatter.png\", title=None, xl=None, yl=None):\n",
    "    x = df[[xcol, ycol]].dropna()\n",
    "    X = x[xcol].values\n",
    "    Y = x[ycol].values\n",
    "\n",
    "    plt.figure(figsize=(6,4.2))\n",
    "    plt.scatter(X, Y, s=8, alpha=0.5)\n",
    "    # LOWESS if available\n",
    "    try:\n",
    "        from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "        z = lowess(Y, X, frac=0.25, it=0, return_sorted=True)\n",
    "        plt.plot(z[:,0], z[:,1])\n",
    "    except Exception:\n",
    "        # Fallback: 3rd-degree polynomial fit\n",
    "        p = np.poly1d(np.polyfit(X, Y, 3))\n",
    "        xs = np.linspace(X.min(), X.max(), 200)\n",
    "        plt.plot(xs, p(xs))\n",
    "\n",
    "    if title: plt.title(title)\n",
    "    if xl: plt.xlabel(xl)\n",
    "    if yl: plt.ylabel(yl)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Helper: bar by aimag (mean baseline consumption, enterprise share)\n",
    "def save_bar_aimag(val_col, fname=\"bar.png\", title=None, ylabel=None):\n",
    "    if \"aimag.x\" not in df.columns:\n",
    "        return\n",
    "    tmp = df[[\"aimag.x\", val_col]].dropna().groupby(\"aimag.x\")[val_col].mean().sort_values()\n",
    "    plt.figure(figsize=(7.5,4.2))\n",
    "    plt.bar(tmp.index.astype(str), tmp.values)\n",
    "    if title: plt.title(title)\n",
    "    plt.xlabel(\"Aimag\")\n",
    "    if ylabel: plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_bar_aimag_share(ind_col, fname=\"bar_share.png\", title=None, ylabel=None):\n",
    "    if \"aimag.x\" not in df.columns:\n",
    "        return\n",
    "    g = df[[\"aimag.x\", ind_col]].dropna()\n",
    "    tmp = g.groupby(\"aimag.x\")[ind_col].mean().sort_values()\n",
    "    plt.figure(figsize=(7.5,4.2))\n",
    "    plt.bar(tmp.index.astype(str), tmp.values)\n",
    "    if title: plt.title(title)\n",
    "    plt.xlabel(\"Aimag\")\n",
    "    if ylabel: plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- Produce figures ----------\n",
    "# Figure 1: Histograms (baseline log consumption, assets log1p)\n",
    "if Y_base_col in df.columns:\n",
    "    save_hist(Y_base_col, bins=30, fname=\"hist_baselineY.png\",\n",
    "              title=\"Baseline Consumption (log)\", xlabel=\"log(consumption)\")\n",
    "\n",
    "if \"log1p_assets_all0\" in df.columns:\n",
    "    save_hist(\"log1p_assets_all0\", bins=30, fname=\"hist_assets_log1p.png\",\n",
    "              title=\"Assets (log1p)\", xlabel=\"log(assets + 1)\")\n",
    "\n",
    "# Figure 2: Enterprise vs non-enterprise distributions\n",
    "if \"enterprise.x\" in df.columns and Y_base_col in df.columns:\n",
    "    # Box/Violin for baseline consumption by enterprise status\n",
    "    save_violin_by(Y_base_col, \"enterprise.x\", fname=\"violin_baselineY_by_enterprise.png\",\n",
    "                   title=\"Baseline Consumption by Enterprise Status\", labels=(\"No enterprise\",\"Has enterprise\"))\n",
    "    save_box_by(Y_base_col, \"enterprise.x\", fname=\"box_baselineY_by_enterprise.png\",\n",
    "                title=\"Baseline Consumption by Enterprise Status\", labels=(\"No enterprise\",\"Has enterprise\"))\n",
    "\n",
    "if \"profit0\" in df.columns and \"enterprise.x\" in df.columns:\n",
    "    # Profit only among enterprise==1 (to avoid many zeros)\n",
    "    sub = df.loc[df[\"enterprise.x\"]==1, \"profit0\"].dropna()\n",
    "    if len(sub) > 0:\n",
    "        plt.figure(figsize=(6,4.2))\n",
    "        plt.hist(sub.values, bins=30)\n",
    "        plt.title(\"Baseline Profits (enterprise households)\")\n",
    "        plt.xlabel(\"profit0\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, \"hist_profit_enterprise.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "# Figure 3: Scatter assets vs baseline log consumption (with smooth)\n",
    "if \"log1p_assets_all0\" in df.columns and Y_base_col in df.columns:\n",
    "    save_scatter_smooth(\"log1p_assets_all0\", Y_base_col,\n",
    "                        fname=\"scatter_assets_vs_baselineY.png\",\n",
    "                        title=\"Assets vs Baseline Consumption\",\n",
    "                        xl=\"log(assets+1)\", yl=\"log(consumption)\")\n",
    "\n",
    "# Figure 4: Aimag bars (means)\n",
    "if \"aimag.x\" in df.columns:\n",
    "    # Mean baseline consumption by aimag\n",
    "    if Y_base_col in df.columns:\n",
    "        save_bar_aimag(Y_base_col, fname=\"bar_aimag_baselineY.png\",\n",
    "                       title=\"Baseline Consumption by Aimag (mean, log)\",\n",
    "                       ylabel=\"log(consumption)\")\n",
    "    # Enterprise share by aimag\n",
    "    if \"enterprise.x\" in df.columns:\n",
    "        save_bar_aimag_share(\"enterprise.x\", fname=\"bar_aimag_enterprise_share.png\",\n",
    "                             title=\"Enterprise Share by Aimag\", ylabel=\"Share\")\n",
    "\n",
    "# Appendix densities: hours and transfers (log1p)\n",
    "def save_density(col, fname):\n",
    "    x = df[col].dropna().values\n",
    "    if len(x) == 0: return\n",
    "    plt.figure(figsize=(6,4.2))\n",
    "    # simple density via hist with many bins; keep neutral style\n",
    "    plt.hist(x, bins=40, density=True)\n",
    "    plt.title(col + \" (density)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, fname), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "for col in [\"hours_ent.x\",\"hours_wage.x\",\"log1p_rec_y0\",\"log1p_gave_y0\"]:\n",
    "    if col in df.columns:\n",
    "        save_density(col, fname=f\"density_{col}.png\")\n",
    "\n",
    "print(f\"[ok] Figures saved to ./{OUT_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics—Outcomes\n",
    "# ================================\n",
    "# Summary stats (mean, SD, N) + histograms for LOGGED outcomes\n",
    "# ================================\n",
    "import os, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "\n",
    "# --- 0) Collect outcome series (log1p scale)\n",
    "# Primary endline and baseline (already log1p if USE_LOG=True in your cleaner)\n",
    "outcomes = {\n",
    "    f\"log1p_{Y_col}\": Y_use,          # endline outcome on log1p scale\n",
    "    f\"log1p_{Y0_col}\": baselineY,     # baseline outcome on log1p scale\n",
    "}\n",
    "\n",
    "# (Optional) also show level-scale histograms by back-transforming log1p\n",
    "MAKE_LEVEL_HISTS = True\n",
    "GROUP_BY_Z = False   # set True to also make summary by treatment assignment Z (ITT)\n",
    "\n",
    "# --- 1) Helpers\n",
    "def freedman_diaconis_bins(x, max_bins=80):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if x.size < 2:\n",
    "        return 10\n",
    "    q75, q25 = np.percentile(x, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    if iqr <= 0:\n",
    "        return min(30, max_bins)\n",
    "    h = 2 * iqr * (x.size ** (-1/3))\n",
    "    if h <= 0:\n",
    "        return min(30, max_bins)\n",
    "    bins = int(np.ceil((x.max() - x.min()) / h))\n",
    "    return max(10, min(bins, max_bins))\n",
    "\n",
    "def to_latex_booktabs(df, caption, label, colformats=None):\n",
    "    df_fmt = df.copy()\n",
    "    out = []\n",
    "    out.append(\"\\\\begin{table}[!htbp]\\\\centering\")\n",
    "    out.append(f\"\\\\caption{{{caption}}}\")\n",
    "    out.append(f\"\\\\label{{{label}}}\")\n",
    "    out.append(\"\\\\begin{threeparttable}\")\n",
    "    # column alignment\n",
    "    cols = \"l\" + \"r\" * (df_fmt.shape[1])\n",
    "    out.append(f\"\\\\begin{{tabular}}{{{cols}}}\")\n",
    "    out.append(\"\\\\toprule\")\n",
    "    out.append(\" & \" + \" & \".join(df_fmt.columns) + \" \\\\\\\\\")\n",
    "    out.append(\"\\\\midrule\")\n",
    "    for idx, row in df_fmt.iterrows():\n",
    "        out.append(str(idx) + \" & \" + \" & \".join(map(str, row.values)) + \" \\\\\\\\\")\n",
    "    out.append(\"\\\\bottomrule\")\n",
    "    out.append(\"\\\\begin{tablenotes}[flushleft]\")\n",
    "    out.append(\"\\\\footnotesize \\\\textit{Notes}: Statistics computed on the log1p scale for outcomes. \"\n",
    "               \"Geometric mean back-transform for levels is $\\\\exp(\\\\mathbb{E}[\\\\log(1+Y)])-1$, \"\n",
    "               \"which is \\\\emph{not} the arithmetic mean of $Y$.\")\n",
    "    out.append(\"\\\\end{tablenotes}\")\n",
    "    out.append(\"\\\\end{threeparttable}\")\n",
    "    out.append(\"\\\\end{table}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def summarize_series(s: pd.Series, name: str):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    n = int(s.notna().sum())\n",
    "    mu = float(s.mean())\n",
    "    sd = float(s.std(ddof=1))\n",
    "    # back-transform geometric mean for levels, given log1p\n",
    "    gmean_level = float(np.expm1(mu))\n",
    "    return {\n",
    "        \"N\": n,\n",
    "        \"Mean (log1p)\": f\"{mu:.3f}\",\n",
    "        \"SD (log1p)\": f\"{sd:.3f}\",\n",
    "        \"Geometric mean (level)\": f\"{gmean_level:.3f}\",\n",
    "    }\n",
    "\n",
    "# --- 2) Build main summary table\n",
    "rows = []\n",
    "for name, s in outcomes.items():\n",
    "    rows.append((name, summarize_series(s, name)))\n",
    "Summary = pd.DataFrame({k: v for k, v in rows}).T[[\"N\",\"Mean (log1p)\",\"SD (log1p)\",\"Geometric mean (level)\"]]\n",
    "\n",
    "Summary.to_csv(\"tables/outcome_summary.csv\")\n",
    "with open(\"tables/outcome_summary.tex\",\"w\") as f:\n",
    "    f.write(to_latex_booktabs(Summary, \"Outcome Summary Statistics (Log Scale)\", \"tab:outcome_summary\"))\n",
    "print(\"[write] tables/outcome_summary.csv, tables/outcome_summary.tex\")\n",
    "\n",
    "# --- 3) Histograms (log scale, and optional level scale)\n",
    "for name, s in outcomes.items():\n",
    "    x = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if x.empty:\n",
    "        print(f\"[hist] {name}: no data to plot.\")\n",
    "        continue\n",
    "    bins = freedman_diaconis_bins(x)\n",
    "    # LOG histogram\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=bins, edgecolor=\"black\", alpha=0.8)\n",
    "    plt.axvline(x.mean(), linestyle=\"--\", linewidth=1)\n",
    "    plt.axvline(np.median(x), linestyle=\":\", linewidth=1)\n",
    "    plt.title(f\"Histogram — {name}\")\n",
    "    plt.xlabel(f\"{name} (log1p scale)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    fn_log = f\"figs/hist_log_{name}.png\".replace(\" \", \"_\")\n",
    "    plt.savefig(fn_log, dpi=300); plt.close()\n",
    "    print(f\"[write] {fn_log}\")\n",
    "    # LEVEL histogram (optional)\n",
    "    if MAKE_LEVEL_HISTS:\n",
    "        x_level = np.expm1(x)\n",
    "        binsL = freedman_diaconis_bins(x_level)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(x_level, bins=binsL, edgecolor=\"black\", alpha=0.8)\n",
    "        plt.axvline(x_level.mean(), linestyle=\"--\", linewidth=1)\n",
    "        plt.axvline(np.median(x_level), linestyle=\":\", linewidth=1)\n",
    "        plt.title(f\"Histogram — {name} (levels)\")\n",
    "        plt.xlabel(f\"{name.replace('log1p_', '')} (level scale)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        fn_lv = f\"figs/hist_level_{name}.png\".replace(\" \", \"_\")\n",
    "        plt.savefig(fn_lv, dpi=300); plt.close()\n",
    "        print(f\"[write] {fn_lv}\")\n",
    "\n",
    "# --- 4) (Optional) By-assignment summary (ITT-flavored descriptive)\n",
    "if GROUP_BY_Z and \"treat_any\" in X_proc.columns:\n",
    "    # Pull Z for the kept rows\n",
    "    Z = X_proc.loc[Y_use.index, \"treat_any\"]\n",
    "    byZ_rows = []\n",
    "    for name, s in outcomes.items():\n",
    "        s_use = pd.to_numeric(s, errors=\"coerce\")\n",
    "        df_tmp = pd.DataFrame({\"y\": s_use, \"Z\": Z}).dropna()\n",
    "        for z in (0.0, 1.0):\n",
    "            y_z = df_tmp.loc[df_tmp[\"Z\"]==z, \"y\"]\n",
    "            n = int(y_z.notna().sum())\n",
    "            mu = float(y_z.mean()); sd = float(y_z.std(ddof=1))\n",
    "            gmean_level = float(np.expm1(mu))\n",
    "            byZ_rows.append({\n",
    "                \"Outcome\": name, \"Group\": f\"Z={int(z)}\",\n",
    "                \"N\": n, \"Mean (log1p)\": f\"{mu:.3f}\",\n",
    "                \"SD (log1p)\": f\"{sd:.3f}\",\n",
    "                \"Geometric mean (level)\": f\"{gmean_level:.3f}\"\n",
    "            })\n",
    "    ByZ = (pd.DataFrame(byZ_rows)\n",
    "             .set_index([\"Outcome\",\"Group\"])\n",
    "             [[\"N\",\"Mean (log1p)\",\"SD (log1p)\",\"Geometric mean (level)\"]])\n",
    "    ByZ.to_csv(\"tables/outcome_summary_byZ.csv\")\n",
    "    # compact LaTeX\n",
    "    lines = []\n",
    "    lines.append(\"\\\\begin{table}[!htbp]\\\\centering\")\n",
    "    lines.append(\"\\\\caption{Outcome Summary by Treatment Assignment (Log Scale)}\")\n",
    "    lines.append(\"\\\\label{tab:outcome_summary_byZ}\")\n",
    "    lines.append(\"\\\\begin{threeparttable}\")\n",
    "    lines.append(\"\\\\begin{tabular}{llrrrr}\")\n",
    "    lines.append(\"\\\\toprule\")\n",
    "    lines.append(\"Outcome & Group & N & Mean (log1p) & SD (log1p) & Geometric mean (level)\\\\\\\\\")\n",
    "    lines.append(\"\\\\midrule\")\n",
    "    for (out, grp), row in ByZ.iterrows():\n",
    "        lines.append(f\"{out} & {grp} & {row['N']} & {row['Mean (log1p)']} & {row['SD (log1p)']} & {row['Geometric mean (level)']}\\\\\\\\\")\n",
    "    lines.append(\"\\\\bottomrule\")\n",
    "    lines.append(\"\\\\begin{tablenotes}[flushleft]\")\n",
    "    lines.append(\"\\\\footnotesize \\\\textit{Notes}: Descriptive only; causal ITT is estimated in regression tables.\")\n",
    "    lines.append(\"\\\\end{tablenotes}\")\n",
    "    lines.append(\"\\\\end{threeparttable}\")\n",
    "    lines.append(\"\\\\end{table}\")\n",
    "    with open(\"tables/outcome_summary_byZ.tex\",\"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(\"[write] tables/outcome_summary_byZ.csv, tables/outcome_summary_byZ.tex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2b31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a31b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diagnostics + explicit output dir for tables ---\n",
    "import os\n",
    "TABLE_DIR = \"tables\"\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"[cwd] {os.getcwd()}\")\n",
    "print(f\"[table] rows={len(table1)}; non-null vars={table1['variable'].nunique()}\")\n",
    "\n",
    "# Save CSV + LaTeX into tables/\n",
    "csv_path = os.path.join(TABLE_DIR, \"table1_baseline.csv\")\n",
    "tex_path = os.path.join(TABLE_DIR, \"table1_baseline.tex\")\n",
    "table1.to_csv(csv_path, index=False)\n",
    "\n",
    "def to_latex_table(tab, fname, caption=\"Baseline Characteristics (Curated)\", label=\"tab:baseline\"):\n",
    "    show = tab[[\"label\",\"Mean (All)\",\"SD (All)\",\"Mean (Control)\",\"Mean (JL Offer)\",\"Std. Diff\",\"N (All)\"]].copy()\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[H]\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{\" + caption + \"}\\n\")\n",
    "        f.write(\"\\\\label{\" + label + \"}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lrrrrrr}\\n\\\\toprule\\n\")\n",
    "        f.write(\"Variable & Mean (All) & SD (All) & Mean (Control) & Mean (JL Offer) & Std. Diff & N \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in show.iterrows():\n",
    "            f.write(f\"{r['label']} & {r['Mean (All)']:.3f} & {r['SD (All)']:.3f} & \"\n",
    "                    f\"{r['Mean (Control)']:.3f} & {r['Mean (JL Offer)']:.3f} & \"\n",
    "                    f\"{r['Std. Diff']:.3f} & {int(r['N (All)'])} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\begin{minipage}{0.92\\\\linewidth}\\\\vspace{0.6ex}\\\\footnotesize\\n\")\n",
    "        f.write(\"Notes: Standardized differences use pooled SD. Heavy-tailed variables are log-transformed in figures. \")\n",
    "        f.write(\"Village-level clustering is used for inference in subsequent sections.\\n\")\n",
    "        f.write(\"\\\\end{minipage}\\n\\\\end{table}\\n\")\n",
    "\n",
    "to_latex_table(table1, tex_path)\n",
    "print(f\"[ok] wrote {csv_path}\")\n",
    "print(f\"[ok] wrote {tex_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cce7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Table 2 — Outcomes (baseline & endline) with cluster-robust ITT\n",
    "# ===========================================\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ---------- ensure output dir ----------\n",
    "TABLE_DIR = \"tables\"\n",
    "os.makedirs(TABLE_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def pick_first_exist(cands, cols, label):\n",
    "    for c in cands:\n",
    "        if c in cols:\n",
    "            print(f\"[choose] {label}: '{c}'\")\n",
    "            return c\n",
    "    print(f\"[skip] none found for {label} among {cands}\")\n",
    "    return None\n",
    "\n",
    "def std_diff(x_t, x_c):\n",
    "    mt, mc = np.nanmean(x_t), np.nanmean(x_c)\n",
    "    vt, vc = np.nanvar(x_t, ddof=1), np.nanvar(x_c, ddof=1)\n",
    "    if not np.isfinite(vt) or not np.isfinite(vc):\n",
    "        return np.nan\n",
    "    sp = math.sqrt((vt + vc) / 2.0)\n",
    "    return (mt - mc) / sp if sp > 0 else np.nan\n",
    "\n",
    "def find_cluster_col(cols):\n",
    "    for c in [\"village_id\",\"village\",\"cluster_id\",\"cluster\",\"psu\",\"psu_id\",\"community\",\"community_id\",\"rescode\",\"rescode.x\",\"rescode.y\"]:\n",
    "        if c in cols:\n",
    "            print(f\"[choose] cluster: '{c}'\")\n",
    "            return c\n",
    "    print(\"[warn] no cluster id column found; will use HC2 robust SEs.\")\n",
    "    return None\n",
    "\n",
    "def itt_cluster_robust(df_run, ycol, tcol, cluster_col=None):\n",
    "    y = pd.to_numeric(df_run[ycol], errors=\"coerce\")\n",
    "    d = pd.to_numeric(df_run[tcol], errors=\"coerce\")\n",
    "    ok = y.notna() & d.notna()\n",
    "    if ok.sum() == 0 or pd.Series(d[ok]).nunique() < 2:\n",
    "        return np.nan, np.nan\n",
    "    X = sm.add_constant(d[ok].to_numpy())\n",
    "    yv = y[ok].to_numpy()\n",
    "    model = sm.OLS(yv, X, missing='drop')\n",
    "    if cluster_col is not None and (cluster_col in df_run.columns):\n",
    "        g = df_run.loc[ok, cluster_col]\n",
    "        fit = model.fit(cov_type='cluster', cov_kwds={'groups': g})\n",
    "    else:\n",
    "        fit = model.fit(cov_type='HC2')\n",
    "    return float(fit.params[1]), float(fit.bse[1])\n",
    "\n",
    "# ---------- label map ----------\n",
    "LABELS_OUT = {\n",
    "    \"Y0_main\": \"Baseline consumption (log1p)\",\n",
    "    \"Y1_main\": \"Endline consumption (log1p)\",\n",
    "    \"food0\":   \"Baseline food exp. (log1p)\",\n",
    "    \"food1\":   \"Endline food exp. (log1p)\",\n",
    "    \"profit0\": \"Baseline business profit (levels)\",\n",
    "    \"profit1\": \"Endline business profit (levels)\",\n",
    "    \"enterprise0\": \"Baseline enterprise (0/1)\",\n",
    "    \"enterprise1\": \"Endline enterprise (0/1)\",\n",
    "}\n",
    "\n",
    "# ---------- build outcomes list from what actually exists ----------\n",
    "outcomes = []\n",
    "\n",
    "# Required: main consumption\n",
    "if \"baseliney\" in df.columns:\n",
    "    outcomes.append((\"Y0_main\", \"baseliney\", True))\n",
    "else:\n",
    "    # build it from baseline column if present\n",
    "    Y0_candidates = [\"totalc0\",\"consumption0\",\"totalc_base\",\"cons_total0\",\"consumption_baseline\"]\n",
    "    Y0_col = pick_first_exist(Y0_candidates, df.columns, \"baseline outcome Y0 (for baseliney)\")\n",
    "    if Y0_col:\n",
    "        df[\"baseliney\"] = np.log1p(pd.to_numeric(df[Y0_col], errors=\"coerce\"))\n",
    "        outcomes.append((\"Y0_main\", \"baseliney\", True))\n",
    "\n",
    "# Endline main\n",
    "if \"log_\" + Y_col in df.columns:\n",
    "    outcomes.append((\"Y1_main\", \"log_\" + Y_col, False))\n",
    "else:\n",
    "    df[\"log_\" + Y_col] = np.log1p(pd.to_numeric(df[Y_col], errors=\"coerce\"))\n",
    "    outcomes.append((\"Y1_main\", \"log_\" + Y_col, False))\n",
    "\n",
    "# Optional: food consumption baseline/endline\n",
    "food_base = pick_first_exist([\"foodc0\",\"food0\",\"food_base\"], df.columns, \"baseline food\")\n",
    "if food_base:\n",
    "    df[\"log1p_\"+food_base] = np.log1p(pd.to_numeric(df[food_base], errors=\"coerce\"))\n",
    "    outcomes.append((\"food0\", \"log1p_\"+food_base, True))\n",
    "\n",
    "food_end = pick_first_exist([\"foodc1\",\"foodc_end\",\"food1\",\"cons_food1\",\"cons_food_end\"], df.columns, \"endline food\")\n",
    "if food_end:\n",
    "    df[\"log1p_\"+food_end] = np.log1p(pd.to_numeric(df[food_end], errors=\"coerce\"))\n",
    "    outcomes.append((\"food1\", \"log1p_\"+food_end, False))\n",
    "\n",
    "# Optional: profits baseline/endline (levels)\n",
    "if \"profit0\" in df.columns:\n",
    "    df[\"profit0\"] = pd.to_numeric(df[\"profit0\"], errors=\"coerce\")\n",
    "    outcomes.append((\"profit0\",\"profit0\", True))\n",
    "profit1 = pick_first_exist([\"profit1\",\"profit_end\",\"profits1\",\"profits_end\"], df.columns, \"endline profit\")\n",
    "if profit1:\n",
    "    df[profit1] = pd.to_numeric(df[profit1], errors=\"coerce\")\n",
    "    outcomes.append((\"profit1\", profit1, False))\n",
    "\n",
    "# Optional: enterprise indicator baseline/endline\n",
    "ent0 = pick_first_exist([\"enterprise0\",\"enterprise_base\",\"enterprise.x\"], df.columns, \"baseline enterprise\")\n",
    "if ent0:\n",
    "    outcomes.append((\"enterprise0\", ent0, True))\n",
    "ent1 = pick_first_exist([\"enterprise1\",\"enterprise_end\",\"enterprise.y\"], df.columns, \"endline enterprise\")\n",
    "if ent1:\n",
    "    outcomes.append((\"enterprise1\", ent1, False))\n",
    "\n",
    "# If nothing made it in:\n",
    "if len(outcomes) == 0:\n",
    "    raise RuntimeError(\"[abort] No outcomes detected. Check column names and re-run.\")\n",
    "\n",
    "# ---------- cluster col ----------\n",
    "cluster_col = find_cluster_col(df.columns)\n",
    "\n",
    "# ---------- compute table ----------\n",
    "rows = []\n",
    "missing_cols = []\n",
    "for code, col, is_base in outcomes:\n",
    "    if col not in df.columns:\n",
    "        missing_cols.append(col)\n",
    "        continue\n",
    "\n",
    "    sub = df[[col, \"treat_group\"]].copy()\n",
    "    sub[\"treat_group\"] = pd.to_numeric(sub[\"treat_group\"], errors=\"coerce\")\n",
    "    sub = sub.dropna(subset=[col, \"treat_group\"])\n",
    "    if sub.empty:\n",
    "        print(f\"[skip] outcome '{col}' has no usable data after dropping NA.\")\n",
    "        continue\n",
    "\n",
    "    all_mean = sub[col].mean()\n",
    "    all_sd   = sub[col].std(ddof=1)\n",
    "    m_c      = sub.loc[sub[\"treat_group\"]==0, col].mean()\n",
    "    m_t      = sub.loc[sub[\"treat_group\"]==1, col].mean()\n",
    "\n",
    "    sdiff = np.nan\n",
    "    if is_base:\n",
    "        sdiff = std_diff(sub.loc[sub[\"treat_group\"]==1, col],\n",
    "                         sub.loc[sub[\"treat_group\"]==0, col])\n",
    "\n",
    "    # ITT with robust SE\n",
    "    if cluster_col:\n",
    "        itt, se = itt_cluster_robust(df[[col,\"treat_group\",cluster_col]], col, \"treat_group\", cluster_col)\n",
    "    else:\n",
    "        itt, se = itt_cluster_robust(df[[col,\"treat_group\"]], col, \"treat_group\", None)\n",
    "\n",
    "    rows.append({\n",
    "        \"Outcome\": LABELS_OUT.get(code, code),\n",
    "        \"N\": int(sub.shape[0]),\n",
    "        \"Mean (All)\": all_mean,\n",
    "        \"SD (All)\": all_sd,\n",
    "        \"Mean (Control)\": m_c,\n",
    "        \"Mean (JL Offer)\": m_t,\n",
    "        \"ITT (JL − C)\": itt,\n",
    "        \"SE (robust)\": se,\n",
    "        \"Std. Diff (baseline)\": sdiff\n",
    "    })\n",
    "\n",
    "if missing_cols:\n",
    "    print(\"[warn] Missing columns skipped:\", missing_cols)\n",
    "\n",
    "tab2 = pd.DataFrame(rows)\n",
    "\n",
    "# guard: if empty, print diagnostics and stop gracefully\n",
    "if tab2.empty:\n",
    "    print(\"[warn] No rows produced for outcomes table — nothing to write.\")\n",
    "else:\n",
    "    # optional: order rows in a pleasing way without relying on 'Code'\n",
    "    order_pref = [\n",
    "        \"Baseline consumption (log1p)\",\n",
    "        \"Endline consumption (log1p)\",\n",
    "        \"Baseline food exp. (log1p)\",\n",
    "        \"Endline food exp. (log1p)\",\n",
    "        \"Baseline business profit (levels)\",\n",
    "        \"Endline business profit (levels)\",\n",
    "        \"Baseline enterprise (0/1)\",\n",
    "        \"Endline enterprise (0/1)\",\n",
    "    ]\n",
    "    tab2[\"ord\"] = tab2[\"Outcome\"].apply(lambda x: order_pref.index(x) if x in order_pref else 999)\n",
    "    tab2 = tab2.sort_values([\"ord\",\"Outcome\"]).drop(columns=[\"ord\"])\n",
    "\n",
    "    # write CSV\n",
    "    csv_path = os.path.join(TABLE_DIR, \"table2_outcomes.csv\")\n",
    "    tab2.to_csv(csv_path, index=False)\n",
    "    print(f\"[ok] wrote {csv_path} (rows={len(tab2)})\")\n",
    "\n",
    "    # write LaTeX\n",
    "    tex_path = os.path.join(TABLE_DIR, \"table2_outcomes.tex\")\n",
    "    with open(tex_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[H]\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{Outcomes: Baseline/Endline Means and ITT (cluster-robust)}\\n\")\n",
    "        f.write(\"\\\\label{tab:outcomes}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{lrrrrrrrr}\\n\\\\toprule\\n\")\n",
    "        f.write(\"Outcome & N & Mean (All) & SD (All) & Mean (Control) & Mean (JL Offer) & ITT (JL$-$C) & SE & Std. Diff (baseline) \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in tab2.iterrows():\n",
    "            f.write(\n",
    "                f\"{r['Outcome']} & {r['N']:d} & \"\n",
    "                f\"{r['Mean (All)']:.3f} & {r['SD (All)']:.3f} & \"\n",
    "                f\"{r['Mean (Control)']:.3f} & {r['Mean (JL Offer)']:.3f} & \"\n",
    "                f\"{(r['ITT (JL − C)'] if pd.notna(r['ITT (JL − C)']) else float('nan')):.3f} & \"\n",
    "                f\"{(r['SE (robust)'] if pd.notna(r['SE (robust)']) else float('nan')):.3f} & \"\n",
    "                f\"{(r['Std. Diff (baseline)'] if pd.notna(r['Std. Diff (baseline)']) else float('nan')):.3f} \\\\\\\\\\n\"\n",
    "            )\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\")\n",
    "        f.write(\"\\\\begin{minipage}{0.95\\\\linewidth}\\\\vspace{0.6ex}\\\\footnotesize\\n\")\n",
    "        f.write(\"Notes: ITT estimated by OLS of outcome on a joint-liability offer indicator with village-clustered standard errors when a cluster id is available; otherwise HC2 robust SEs. \")\n",
    "        f.write(\"Consumption and food outcomes are log1p-transformed; profits are in levels unless otherwise noted. \")\n",
    "        f.write(\"Standardized differences are shown only for baseline outcomes as a balance diagnostic. \")\n",
    "        f.write(\"\\\\end{minipage}\\n\\\\end{table}\\n\")\n",
    "    print(f\"[ok] wrote {tex_path}\")\n",
    "\n",
    "# In LaTeX: \\input{tables/table2_outcomes.tex}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243f1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c537733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e697b2",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe54bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ANCOVA: y1 ~ y0 + Treatment + ALL baseline controls\n",
    "# Showing Coefficients for All Control Variables\n",
    "# Robust to dtype/object issues\n",
    "# ==========================================\n",
    "import numpy as np, pandas as pd, statsmodels.api as sm\n",
    "import os\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "# 0) Align unscaled feature matrix to Y rows\n",
    "X_unscaled = X_proc.loc[Y_use.index].copy()   # pre-scale, imputed + one-hot\n",
    "\n",
    "# 1) Column lists\n",
    "drop_keys = {\"treat_any\",\"treat_indiv\",\"treat_group\",\"baselineY\"}\n",
    "baseline_controls = [c for c in X_unscaled.columns if c not in drop_keys]\n",
    "\n",
    "cols_any = [\"baselineY\",\"treat_any\"] + baseline_controls\n",
    "cols_two = [\"baselineY\",\"treat_indiv\",\"treat_group\"] + baseline_controls\n",
    "\n",
    "# 2) Helper: make design numeric and clean\n",
    "def make_numeric_design(df, cols):\n",
    "    X = df[cols].copy()\n",
    "\n",
    "    # De-duplicate columns if any (keep first occurrence)\n",
    "    if X.columns.duplicated().any():\n",
    "        dup = X.columns[X.columns.duplicated()].tolist()\n",
    "        print(f\"[warn] dropping duplicate columns: {dup[:8]}{'...' if len(dup)>8 else ''}\")\n",
    "        X = X.loc[:, ~X.columns.duplicated()]\n",
    "\n",
    "    # Cast booleans → int8\n",
    "    bool_cols = [c for c in X.columns if X[c].dtype == bool]\n",
    "    if bool_cols:\n",
    "        X[bool_cols] = X[bool_cols].astype(\"int8\")\n",
    "\n",
    "    # Category → codes (missing becomes NaN, fill later)\n",
    "    cat_cols = [c for c in X.columns if str(X[c].dtype) == \"category\"]\n",
    "    for c in cat_cols:\n",
    "        X[c] = X[c].cat.codes.replace(-1, np.nan)\n",
    "\n",
    "    # Object → numeric (coerce)\n",
    "    obj_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
    "    if obj_cols:\n",
    "        print(f\"[fix] coercing object columns to numeric: {obj_cols[:8]}{'...' if len(obj_cols)>8 else ''}\")\n",
    "        for c in obj_cols:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "    # Replace inf, then fill remaining NaN with column mean\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].fillna(X[c].mean())\n",
    "\n",
    "    # Final cast to float64\n",
    "    X = X.astype(\"float64\")\n",
    "\n",
    "    # Sanity check: any non-numeric left?\n",
    "    bad = [c for c in X.columns if not np.issubdtype(X[c].dtype, np.number)]\n",
    "    assert not bad, f\"Non-numeric columns remain: {bad}\"\n",
    "    return X\n",
    "\n",
    "X_any = make_numeric_design(X_unscaled, cols_any)\n",
    "X_two = make_numeric_design(X_unscaled, cols_two)\n",
    "\n",
    "# 3) Cluster variable (village-level)\n",
    "cluster_candidates = [\"village_id\",\"cluster_id\",\"community_id\",\"village\",\"cluster\",\"soum\",\"aimag\"]\n",
    "cluster_col = next((c for c in cluster_candidates if c in Merged.columns), None)\n",
    "clusters = Merged.loc[Y_use.index, cluster_col] if cluster_col else pd.Series(np.arange(len(Y_use)), index=Y_use.index)\n",
    "\n",
    "# 4) Fit function with clustered SEs\n",
    "def fit_ancova(y, X, clusters):\n",
    "    y = np.asarray(y, dtype=\"float64\")  # ensure numeric\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    model = sm.OLS(y, X, missing=\"drop\")\n",
    "    res = model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "    return res\n",
    "\n",
    "res_any = fit_ancova(Y_use, X_any, clusters)\n",
    "res_two = fit_ancova(Y_use, X_two, clusters)\n",
    "\n",
    "print(\"\\n=== ANCOVA (Any offer) ===\")\n",
    "print(res_any.summary())\n",
    "\n",
    "print(\"\\n=== ANCOVA (Two arms: individual & joint liability) ===\")\n",
    "print(res_two.summary())\n",
    "\n",
    "# 5) Save key coefficients for quick tables\n",
    "pd.DataFrame({\n",
    "    \"coef\": res_any.params[[\"treat_any\",\"baselineY\"]],\n",
    "    \"se_cluster\": res_any.bse[[\"treat_any\",\"baselineY\"]],\n",
    "    \"pval\": res_any.pvalues[[\"treat_any\",\"baselineY\"]],\n",
    "}).to_csv(\"tables/ancova_any_offer_key.csv\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"coef\": res_two.params[[\"treat_indiv\",\"treat_group\",\"baselineY\"]],\n",
    "    \"se_cluster\": res_two.bse[[\"treat_indiv\",\"treat_group\",\"baselineY\"]],\n",
    "    \"pval\": res_two.pvalues[[\"treat_indiv\",\"treat_group\",\"baselineY\"]],\n",
    "}).to_csv(\"tables/ancova_two_arm_key.csv\")\n",
    "\n",
    "print(\"[write] tables/ancova_any_offer_key.csv, tables/ancova_two_arm_key.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edccc26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ae3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:1884: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(np.diag(self.cov_params()))\n",
      "/opt/conda/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:1884: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(np.diag(self.cov_params()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[write] tables/ancova_main.tex, tables/ancova_main.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# OLS (ANCOVA), report-only table: Constant, BaselineY, Treatment(s)\n",
    "# ==========================================\n",
    "import os, numpy as np, pandas as pd, statsmodels.api as sm\n",
    "\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "# 0) Align to kept rows and use the unscaled, imputed, one-hot matrix\n",
    "X_unscaled = X_proc.loc[Y_use.index].copy()\n",
    "\n",
    "# 1) Build column sets\n",
    "drop_keys = {\"treat_any\",\"treat_indiv\",\"treat_group\",\"baselineY\"}\n",
    "baseline_controls = [c for c in X_unscaled.columns if c not in drop_keys]\n",
    "\n",
    "cols_any = [\"baselineY\",\"treat_any\"] + baseline_controls\n",
    "cols_two = [\"baselineY\",\"treat_indiv\",\"treat_group\"] + baseline_controls\n",
    "\n",
    "# 2) Clean numeric design (handles object/booleans, fills NaN with mean)\n",
    "def make_numeric_design(df, cols):\n",
    "    X = df[cols].copy()\n",
    "    # dedupe\n",
    "    if X.columns.duplicated().any():\n",
    "        X = X.loc[:, ~X.columns.duplicated()]\n",
    "    # bool -> int8\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == bool:\n",
    "            X[c] = X[c].astype(\"int8\")\n",
    "    # category -> codes\n",
    "    for c in X.columns:\n",
    "        if str(X[c].dtype) == \"category\":\n",
    "            X[c] = X[c].cat.codes.replace(-1, np.nan)\n",
    "    # object -> numeric\n",
    "    obj_cols = [c for c in X.columns if X[c].dtype == \"O\"]\n",
    "    for c in obj_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    # fill and cast\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].fillna(X[c].mean())\n",
    "    return X.astype(\"float64\")\n",
    "\n",
    "X_any = make_numeric_design(X_unscaled, cols_any)\n",
    "X_two = make_numeric_design(X_unscaled, cols_two)\n",
    "\n",
    "# 3) Cluster ids (village-level if available)\n",
    "cluster_candidates = [\"village_id\",\"cluster_id\",\"community_id\",\"village\",\"cluster\",\"soum\",\"aimag\"]\n",
    "cluster_col = next((c for c in cluster_candidates if c in Merged.columns), None)\n",
    "clusters = (Merged.loc[Y_use.index, cluster_col]\n",
    "            if cluster_col else pd.Series(np.arange(len(Y_use)), index=Y_use.index))\n",
    "\n",
    "# 4) Fit with village-clustered SEs\n",
    "def fit_clustered(y, X, clusters):\n",
    "    y = np.asarray(y, dtype=\"float64\")\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    model = sm.OLS(y, X, missing=\"drop\")\n",
    "    return model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "\n",
    "res_any = fit_clustered(Y_use, X_any, clusters)\n",
    "res_two = fit_clustered(Y_use, X_two, clusters)\n",
    "\n",
    "# 5) Build a compact report table (only selected rows)\n",
    "def stars(p):\n",
    "    return \"\" if p>=0.1 else \"*\" if p>=0.05 else \"**\" if p>=0.01 else \"***\"\n",
    "\n",
    "def pick_rows(res, wanted):\n",
    "    b, se, p = res.params, res.bse, res.pvalues\n",
    "    out = []\n",
    "    for lab, key in wanted:\n",
    "        if key not in b.index and key is not None:\n",
    "            out.append((lab, \"\"))  # blank if not in this spec\n",
    "        elif key is None:  # constant row uses 'const'\n",
    "            val = b.get(\"const\", np.nan); s = se.get(\"const\", np.nan); pv = p.get(\"const\", 1.0)\n",
    "            out.append((lab, f\"{val:0.3f}{stars(pv)}\\n({s:0.3f})\"))\n",
    "        else:\n",
    "            val = b[key]; s = se[key]; pv = p[key]\n",
    "            out.append((lab, f\"{val:0.3f}{stars(pv)}\\n({s:0.3f})\"))\n",
    "    return out\n",
    "\n",
    "# Rows to display (order)\n",
    "rows_any = [(\"Treatment (Any offer)\", \"treat_any\"),\n",
    "            (\"Baseline outcome\",       \"baselineY\"),\n",
    "            (\"Constant\",               None)]\n",
    "rows_two = [(\"Treatment: Individual\", \"treat_indiv\"),\n",
    "            (\"Treatment: Joint\",      \"treat_group\"),\n",
    "            (\"Baseline outcome\",      \"baselineY\"),\n",
    "            (\"Constant\",              None)]\n",
    "\n",
    "col_any  = pick_rows(res_any, rows_any)\n",
    "col_two  = pick_rows(res_two, rows_two)\n",
    "\n",
    "# 6) Assemble into a DataFrame for LaTeX\n",
    "# Union the row labels in a sensible order\n",
    "index = [r[0] for r in rows_two]  # superset order\n",
    "report = pd.DataFrame(index=index, columns=[\"(1) Any offer\",\"(2) Individual/Joint\"])\n",
    "# fill column (1)\n",
    "for lab, val in col_any:\n",
    "    if lab not in report.index:\n",
    "        report.loc[lab] = [\"\", \"\"]\n",
    "    report.loc[lab, \"(1) Any offer\"] = val\n",
    "# fill column (2)\n",
    "for lab, val in col_two:\n",
    "    if lab not in report.index:\n",
    "        report.loc[lab] = [\"\", \"\"]\n",
    "    report.loc[lab, \"(2) Individual/Joint\"] = val\n",
    "\n",
    "# Footer stats\n",
    "footer = pd.DataFrame({\n",
    "    \"(1) Any offer\":        [int(res_any.nobs),  res_any.rsquared],\n",
    "    \"(2) Individual/Joint\": [int(res_two.nobs),  res_two.rsquared],\n",
    "}, index=[\"Observations\",\"R-squared\"])\n",
    "\n",
    "# 7) Minimal LaTeX (no threeparttable). Booktabs optional.\n",
    "use_booktabs = True\n",
    "def to_latex_min(tbl, foot, dep_label, controls_note, cluster_name):\n",
    "    cols_spec = \"l\" + \"c\"*tbl.shape[1]\n",
    "    lines = []\n",
    "    lines.append(\"\\\\begin{table}[!htbp]\\\\centering\")\n",
    "    lines.append(\"\\\\caption{ANCOVA: Outcome on Baseline, Treatment, and All Baseline Controls}\")\n",
    "    lines.append(\"\\\\label{tab:ancova_main}\")\n",
    "    if use_booktabs:\n",
    "        lines.append(\"\\\\begin{tabular}{\"+cols_spec+\"}\")\n",
    "        lines.append(\"\\\\toprule\")\n",
    "    else:\n",
    "        lines.append(\"\\\\begin{tabular}{\"+cols_spec+\"}\")\n",
    "        lines.append(\"\\\\hline\")\n",
    "    lines.append(\" & \" + \" & \".join(tbl.columns) + \" \\\\\\\\\")\n",
    "    lines.append(\"\\\\midrule\" if use_booktabs else \"\\\\hline\")\n",
    "    for r in tbl.index:\n",
    "        c1 = tbl.loc[r, tbl.columns[0]] if pd.notna(tbl.loc[r, tbl.columns[0]]) else \"\"\n",
    "        c2 = tbl.loc[r, tbl.columns[1]] if pd.notna(tbl.loc[r, tbl.columns[1]]) else \"\"\n",
    "        lines.append(f\"{r} & {c1} & {c2} \\\\\\\\\")\n",
    "    lines.append(\"\\\\midrule\" if use_booktabs else \"\\\\hline\")\n",
    "    lines.append(f\"Observations & {foot.loc['Observations','(1) Any offer']} & {foot.loc['Observations','(2) Individual/Joint']} \\\\\\\\\")\n",
    "    lines.append(f\"R-squared & {foot.loc['R-squared','(1) Any offer']:.3f} & {foot.loc['R-squared','(2) Individual/Joint']:.3f} \\\\\\\\\")\n",
    "    lines.append(\"\\\\bottomrule\" if use_booktabs else \"\\\\hline\")\n",
    "    lines.append(\"\\\\end{tabular}\")\n",
    "    # Plain minipage note\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"\\\\par\\\\vspace{0.5ex}\")\n",
    "    lines.append(\"\\\\begin{minipage}{0.94\\\\linewidth}\\\\footnotesize\")\n",
    "    lines.append(f\"Notes: Dependent variable {dep_label}. Heteroskedasticity- and cluster-robust standard errors in parentheses (clustered by {cluster_name}). \")\n",
    "    lines.append(\"All specifications include the full set of baseline controls (unreported) and province/aimag fixed effects if present in the baseline set. \")\n",
    "    lines.append(\"*, **, *** denote $p<0.05, p<0.01, p<0.001$, respectively.\")\n",
    "    lines.append(\"\\\\end{minipage}\")\n",
    "    lines.append(\"\\\\end{table}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "dep_label = f\"$\\\\log(1+{Y_col})$\"\n",
    "cluster_name = cluster_col if cluster_col else \"village\"\n",
    "latex_str = to_latex_min(report, footer, dep_label,\n",
    "                         controls_note=\"All baseline controls included.\",\n",
    "                         cluster_name=cluster_name)\n",
    "\n",
    "with open(\"tables/ancova_main.tex\",\"w\") as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "# Also save a CSV copy (handy for quick checks)\n",
    "report.assign(**{\"Obs (1)\": int(res_any.nobs),\n",
    "                 \"Obs (2)\": int(res_two.nobs)}).to_csv(\"tables/ancova_main.csv\")\n",
    "\n",
    "print(\"[write] tables/ancova_main.tex, tables/ancova_main.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29d1566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b5a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa208f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:117: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:117: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:117: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:117: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/tmp/ipykernel_1218/1348326420.py:117: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  L += [r\"\\begin{table}[!htbp]\\centering\", f\"\\caption{{{caption}}}\", f\"\\label{{{label}}}\", r\"\\small\",\n",
      "/tmp/ipykernel_1218/1348326420.py:117: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  L += [r\"\\begin{table}[!htbp]\\centering\", f\"\\caption{{{caption}}}\", f\"\\label{{{label}}}\", r\"\\small\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cov] Using cov_type='HC1' (HC1 robust)\n",
      "\n",
      "=== ANCOVA (reporting subset) ===\n",
      "                          (1) Any offer (2) Individual/Joint\n",
      "Constant               18.761\\n(19.744)     18.600\\n(19.793)\n",
      "Baseline outcome         0.336\\n(0.180)       0.331\\n(0.181)\n",
      "Treatment: Individual               NaN       0.012\\n(0.063)\n",
      "Treatment: Joint                    NaN      -0.034\\n(0.068)\n",
      "Treatment (Any offer)   -0.005\\n(0.059)                  NaN\n",
      "\n",
      "=== Footer ===\n",
      "              (1) Any offer  (2) Individual/Joint\n",
      "Observations       961.0000            961.000000\n",
      "R-squared            0.9429              0.943014\n",
      "\n",
      "[write] tables/ancova_main.tex, tables/ancova_main.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Baseline OLS (ANCOVA) — simple & numeric-safe\n",
    "# Shows only: Constant, baselineY, Treatment(s)\n",
    "# Keeps all other baseline features as controls (unreported)\n",
    "# ==========================================\n",
    "import numpy as np, pandas as pd, statsmodels.api as sm, os\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "assert {\"X_proc\",\"Y_use\",\"Merged\"}.issubset(globals()), \"Run your cleaning cell first.\"\n",
    "\n",
    "# ---- 0) Align rows\n",
    "idx = X_proc.index.intersection(Y_use.index).intersection(Merged.index)\n",
    "Y = Y_use.loc[idx].astype(float)\n",
    "X_all = X_proc.loc[idx].copy()\n",
    "\n",
    "# ---- 1) Build two specs (any-offer; two-arm)\n",
    "focus_any = [c for c in [\"baselineY\",\"treat_any\"] if c in X_all.columns]\n",
    "focus_two = [\"baselineY\"] + [c for c in [\"treat_indiv\",\"treat_group\"] if c in X_all.columns]\n",
    "\n",
    "controls = X_all.columns.difference(set(focus_any + focus_two))  # all other baseline controls\n",
    "\n",
    "X_any = pd.concat([X_all[focus_any], X_all[controls]], axis=1)\n",
    "X_two = pd.concat([X_all[focus_two], X_all[controls]], axis=1)\n",
    "\n",
    "# ---- 2) Ensure fully numeric design: get_dummies on any non-numeric, cast to float, fill NaN\n",
    "def ensure_numeric_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    # booleans -> ints\n",
    "    for c in X.columns:\n",
    "        if pd.api.types.is_bool_dtype(X[c]):\n",
    "            X[c] = X[c].astype(np.int8)\n",
    "    non_num = [c for c in X.columns if not pd.api.types.is_numeric_dtype(X[c])]\n",
    "    if non_num:\n",
    "        X = pd.concat([X.drop(columns=non_num),\n",
    "                       pd.get_dummies(X[non_num], drop_first=True, dtype=float)], axis=1)\n",
    "    # replace inf and fill NaN, cast to float\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].astype(float).fillna(X[c].mean())\n",
    "        else:\n",
    "            X[c] = X[c].astype(float)\n",
    "    # de-duplicate columns if any\n",
    "    if X.columns.duplicated().any():\n",
    "        X = X.loc[:, ~X.columns.duplicated()]\n",
    "    return X\n",
    "\n",
    "X_any = ensure_numeric_df(X_any)\n",
    "X_two = ensure_numeric_df(X_two)\n",
    "\n",
    "# ---- 3) Choose covariance: cluster by village if sensible, else HC1\n",
    "cluster_candidates = [\"village_id\",\"village\",\"cluster_id\",\"community_id\",\"soum\",\"aimag\"]\n",
    "cluster_col = next((c for c in cluster_candidates if c in Merged.columns), None)\n",
    "cov_type, cov_kw = \"HC1\", {}\n",
    "if cluster_col is not None:\n",
    "    g = Merged.loc[idx, cluster_col]\n",
    "    if g.notna().all() and g.nunique() >= 20:\n",
    "        cov_type, cov_kw = \"cluster\", {\"groups\": g.astype(\"object\").values}\n",
    "print(f\"[cov] Using cov_type='{cov_type}'\" + (f\" clustered by {cluster_col}\" if cov_type=='cluster' else \" (HC1 robust)\"))\n",
    "\n",
    "# ---- 4) Fit helper\n",
    "def fit_ancova(y: pd.Series, X: pd.DataFrame):\n",
    "    X_use = sm.add_constant(X, has_constant=\"add\")  # add intercept\n",
    "    model = sm.OLS(y.values, X_use.values)\n",
    "    res = model.fit(cov_type=cov_type, cov_kwds=cov_kw)\n",
    "    res._xnames = [\"const\"] + list(X.columns)  # keep names for reporting\n",
    "    return res\n",
    "\n",
    "res_any = fit_ancova(Y, X_any)\n",
    "res_two = fit_ancova(Y, X_two)\n",
    "\n",
    "# ---- 5) Compact report: Constant, baselineY, and treatment(s)\n",
    "def stars(p): return \"***\" if p<0.001 else \"**\" if p<0.01 else \"*\" if p<0.05 else \"\"\n",
    "def extract(res, keys):\n",
    "    b  = pd.Series(res.params, index=getattr(res, \"_xnames\"))\n",
    "    se = pd.Series(res.bse,    index=getattr(res, \"_xnames\"))\n",
    "    p  = pd.Series(res.pvalues,index=getattr(res, \"_xnames\"))\n",
    "    out = []\n",
    "    for lab, key in keys:\n",
    "        nm = \"const\" if key is None else key\n",
    "        val = b.get(nm, np.nan); s = se.get(nm, np.nan); pv = p.get(nm, 1.0)\n",
    "        out.append(f\"{val:.3f}{stars(pv)}\\n({s:.3f})\" if pd.notna(val) else \"\")\n",
    "    return out\n",
    "\n",
    "rows_any = [(\"Constant\", None),\n",
    "            (\"Baseline outcome\", \"baselineY\"),\n",
    "            (\"Treatment (Any offer)\", \"treat_any\")]\n",
    "rows_two = [(\"Constant\", None),\n",
    "            (\"Baseline outcome\", \"baselineY\"),\n",
    "            (\"Treatment: Individual\", \"treat_indiv\"),\n",
    "            (\"Treatment: Joint\", \"treat_group\")]\n",
    "\n",
    "col_any = extract(res_any, rows_any)\n",
    "col_two = extract(res_two, rows_two)\n",
    "\n",
    "index = [r[0] for r in rows_two]\n",
    "tbl = pd.DataFrame(index=index, columns=[\"(1) Any offer\",\"(2) Individual/Joint\"])\n",
    "for (lab,_), val in zip(rows_any, col_any): tbl.loc[lab, \"(1) Any offer\"] = val\n",
    "for (lab,_), val in zip(rows_two, col_two): tbl.loc[lab, \"(2) Individual/Joint\"] = val\n",
    "\n",
    "footer = pd.DataFrame({\n",
    "    \"(1) Any offer\":        [int(res_any.nobs),  res_any.rsquared],\n",
    "    \"(2) Individual/Joint\": [int(res_two.nobs),  res_two.rsquared],\n",
    "}, index=[\"Observations\",\"R-squared\"])\n",
    "\n",
    "print(\"\\n=== ANCOVA (reporting subset) ===\"); print(tbl)\n",
    "print(\"\\n=== Footer ===\"); print(footer)\n",
    "\n",
    "# ---- 6) Minimal LaTeX export\n",
    "use_booktabs = True\n",
    "dep_label = f\"$\\\\log(1+{Y.name})$\" if isinstance(Y.name, str) else \"$\\\\log(1+Y)$\"\n",
    "cluster_txt = f\"clustered by {cluster_col}\" if cov_type==\"cluster\" else \"HC1 robust\"\n",
    "\n",
    "def latex_table(tbl, foot, caption=\"ANCOVA: Outcome on Baseline, Treatment, and Baseline Controls\", label=\"tab:ancova_main\"):\n",
    "    cs = \"l\" + \"c\"*tbl.shape[1]\n",
    "    L = []\n",
    "    L += [r\"\\begin{table}[!htbp]\\centering\", f\"\\caption{{{caption}}}\", f\"\\label{{{label}}}\", r\"\\small\",\n",
    "          r\"\\begin{tabular}{\"+cs+r\"}\", (r\"\\toprule\" if use_booktabs else r\"\\hline\"),\n",
    "          \" & \" + \" & \".join(tbl.columns) + r\" \\\\\",\n",
    "          (r\"\\midrule\" if use_booktabs else r\"\\hline\")]\n",
    "    for r in tbl.index:\n",
    "        L.append(f\"{r} & {tbl.loc[r, tbl.columns[0]] or ''} & {tbl.loc[r, tbl.columns[1]] or ''} \\\\\\\\\")\n",
    "    L += [(r\"\\midrule\" if use_booktabs else r\"\\hline\"),\n",
    "          f\"Observations & {foot.loc['Observations','(1) Any offer']} & {foot.loc['Observations','(2) Individual/Joint']} \\\\\\\\\",\n",
    "          f\"R-squared & {foot.loc['R-squared','(1) Any offer']:.3f} & {foot.loc['R-squared','(2) Individual/Joint']:.3f} \\\\\\\\\",\n",
    "          (r\"\\bottomrule\" if use_booktabs else r\"\\hline\"),\n",
    "          r\"\\end{tabular}\", \"\", r\"\\vspace{0.5ex}\",\n",
    "          r\"\\begin{minipage}{0.94\\linewidth}\\footnotesize\",\n",
    "          f\"Notes: Dependent variable {dep_label}. Standard errors are {cluster_txt}. \",\n",
    "          r\"All specifications include the full set of baseline controls (unreported). \",\n",
    "          r\"*, **, *** denote $p<0.05, p<0.01, p<0.001$, respectively.\",\n",
    "          r\"\\end{minipage}\", r\"\\end{table}\"]\n",
    "    return \"\\n\".join(L)\n",
    "\n",
    "with open(\"tables/ancova_main.tex\",\"w\") as f:\n",
    "    f.write(latex_table(tbl, footer))\n",
    "tbl.assign(**{\"Obs (1)\": int(res_any.nobs), \"Obs (2)\": int(res_two.nobs)}).to_csv(\"tables/ancova_main.csv\")\n",
    "print(\"\\n[write] tables/ancova_main.tex, tables/ancova_main.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842c8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a1b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Post-LASSO ANCOVA: y1 ~ y0 + Treatment + (Top-20 LASSO controls only)\n",
    "# ============================================================\n",
    "import os, numpy as np, pandas as pd, statsmodels.api as sm, difflib\n",
    "\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "# 0) Align design rows to the analysis sample\n",
    "X_unscaled = X_proc.loc[Y_use.index].copy()\n",
    "\n",
    "# 1) Top-20 LASSO variables as shown in your figure legend\n",
    "#    (edit here if your labels differ)\n",
    "LASSO_TOP20_RAW = [\n",
    "    \"redmeat2.x\",\"butter_ps0\",\"transer7.x\",\"transer8.x\",\"vehicl11.x\",\n",
    "    \"schoo12.x\",\"cladul12.x\",\"schoo11.x\",\"othecom7.x\",\"butter1.x\",\n",
    "    \"baselineY\",\"milk2.x\",\"othecom8.x\",\"flour2.x\",\"vegetab1.x\",\"rice1.x\",\n",
    "    \"othdair2.x\",\"choco3.x\",\"othdair1.x\",\"recreat7.x\"\n",
    "]\n",
    "\n",
    "# 2) Clean the list: remove 'baselineY' (we add it explicitly), fuzzy-match near-misses\n",
    "ALL_COLS = X_unscaled.columns.tolist()\n",
    "want = [v for v in LASSO_TOP20_RAW if v != \"baselineY\"]\n",
    "\n",
    "present = [v for v in want if v in ALL_COLS]\n",
    "missing = [v for v in want if v not in ALL_COLS]\n",
    "\n",
    "# Try gentle fuzzy matches for missing labels (e.g., tiny typos)\n",
    "matched = {}\n",
    "for v in missing:\n",
    "    cand = difflib.get_close_matches(v, ALL_COLS, n=1, cutoff=0.8)\n",
    "    if cand:\n",
    "        matched[v] = cand[0]\n",
    "        present.append(cand[0])\n",
    "\n",
    "if missing:\n",
    "    still_missing = [v for v in missing if v not in matched]\n",
    "    if still_missing:\n",
    "        print(f\"[postLASSO] WARNING: not found (no close match): {still_missing}\")\n",
    "if matched:\n",
    "    print(\"[postLASSO] Fuzzy-matched controls:\", matched)\n",
    "\n",
    "postlasso_controls = sorted(set(present))  # unique, sorted for reproducibility\n",
    "print(f\"[postLASSO] Using {len(postlasso_controls)} LASSO-selected controls.\")\n",
    "\n",
    "# 3) Build column sets for pooled any-offer and two-arm specs\n",
    "cols_any = [\"baselineY\",\"treat_any\"] + postlasso_controls\n",
    "cols_two = [\"baselineY\",\"treat_indiv\",\"treat_group\"] + postlasso_controls\n",
    "\n",
    "# 4) Make fully numeric design (handles object/booleans; fills NaN with means)\n",
    "def make_numeric_design(df, cols):\n",
    "    X = df[cols].copy()\n",
    "    # De-duplicate columns\n",
    "    if X.columns.duplicated().any():\n",
    "        X = X.loc[:, ~X.columns.duplicated()]\n",
    "    # Cast types\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == bool:\n",
    "            X[c] = X[c].astype(\"int8\")\n",
    "        elif str(X[c].dtype) == \"category\":\n",
    "            X[c] = X[c].cat.codes.replace(-1, np.nan)\n",
    "        elif X[c].dtype == \"O\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    # Clean infinities, fill NAs with means, cast float64\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].fillna(X[c].mean())\n",
    "    return X.astype(\"float64\")\n",
    "\n",
    "X_any = make_numeric_design(X_unscaled, cols_any)\n",
    "X_two = make_numeric_design(X_unscaled, cols_two)\n",
    "\n",
    "# 5) Cluster ids (village-level if available)\n",
    "cluster_candidates = [\"village_id\",\"cluster_id\",\"community_id\",\"village\",\"cluster\",\"soum\",\"aimag\"]\n",
    "cluster_col = next((c for c in cluster_candidates if c in Merged.columns), None)\n",
    "clusters = (Merged.loc[Y_use.index, cluster_col]\n",
    "            if cluster_col else pd.Series(np.arange(len(Y_use)), index=Y_use.index))\n",
    "\n",
    "# 6) Fit OLS with village-clustered SEs\n",
    "def fit_clustered(y, X, clusters):\n",
    "    y = np.asarray(y, dtype=\"float64\")\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    model = sm.OLS(y, X, missing=\"drop\")\n",
    "    return model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "\n",
    "res_any = fit_clustered(Y_use, X_any, clusters)\n",
    "res_two = fit_clustered(Y_use, X_two, clusters)\n",
    "\n",
    "print(\"\\n=== Post-LASSO ANCOVA (Any offer) ===\")\n",
    "print(res_any.summary())\n",
    "print(\"\\n=== Post-LASSO ANCOVA (Two arms) ===\")\n",
    "print(res_two.summary())\n",
    "\n",
    "# 7) Build a compact report table (only Treatment(s), BaselineY, Constant)\n",
    "def stars(p):\n",
    "    return \"\" if p>=0.1 else \"*\" if p>=0.05 else \"**\" if p>=0.01 else \"***\"\n",
    "\n",
    "def pick_rows(res, wanted):\n",
    "    b, se, p = res.params, res.bse, res.pvalues\n",
    "    out = []\n",
    "    for lab, key in wanted:\n",
    "        if key is None:\n",
    "            val = b.get(\"const\", np.nan); s = se.get(\"const\", np.nan); pv = p.get(\"const\", 1.0)\n",
    "        else:\n",
    "            val = b[key]; s = se[key]; pv = p[key]\n",
    "        out.append((lab, f\"{val:0.3f}{stars(pv)}\\n({s:0.3f})\"))\n",
    "    return out\n",
    "\n",
    "rows_any = [(\"Treatment (Any offer)\",\"treat_any\"),\n",
    "            (\"Baseline outcome\",\"baselineY\"),\n",
    "            (\"Constant\",None)]\n",
    "rows_two = [(\"Treatment: Individual\",\"treat_indiv\"),\n",
    "            (\"Treatment: Joint\",\"treat_group\"),\n",
    "            (\"Baseline outcome\",\"baselineY\"),\n",
    "            (\"Constant\",None)]\n",
    "\n",
    "col_any = pick_rows(res_any, rows_any)\n",
    "col_two = pick_rows(res_two, rows_two)\n",
    "\n",
    "index = [r[0] for r in rows_two]\n",
    "report = pd.DataFrame(index=index, columns=[\"(1) Any offer\",\"(2) Individual/Joint\"])\n",
    "for lab, val in col_any:\n",
    "    report.loc[lab, \"(1) Any offer\"] = val\n",
    "for lab, val in col_two:\n",
    "    report.loc[lab, \"(2) Individual/Joint\"] = val\n",
    "\n",
    "footer = pd.DataFrame({\n",
    "    \"(1) Any offer\":[int(res_any.nobs), res_any.rsquared],\n",
    "    \"(2) Individual/Joint\":[int(res_two.nobs), res_two.rsquared],\n",
    "}, index=[\"Observations\",\"R-squared\"])\n",
    "\n",
    "# 8) Emit LaTeX (post-LASSO caption + note)\n",
    "use_booktabs = True\n",
    "def to_latex_min(tbl, foot, dep_label, cluster_name, k_controls):\n",
    "    cols_spec = \"l\" + \"c\"*tbl.shape[1]\n",
    "    L = []\n",
    "    L.append(\"\\\\begin{table}[!htbp]\\\\centering\")\n",
    "    L.append(\"\\\\caption{Post-LASSO ANCOVA: Outcome on Baseline, Treatment, and Selected Controls}\")\n",
    "    L.append(\"\\\\label{tab:ancova_postlasso}\")\n",
    "    L.append(\"\\\\begin{tabular}{\"+cols_spec+\"}\")\n",
    "    L.append(\"\\\\toprule\" if use_booktabs else \"\\\\hline\")\n",
    "    L.append(\" & \" + \" & \".join(tbl.columns) + \" \\\\\\\\\")\n",
    "    L.append(\"\\\\midrule\" if use_booktabs else \"\\\\hline\")\n",
    "    for r in tbl.index:\n",
    "        L.append(f\"{r} & {tbl.loc[r, tbl.columns[0]] or ''} & {tbl.loc[r, tbl.columns[1]] or ''} \\\\\\\\\")\n",
    "    L.append(\"\\\\midrule\" if use_booktabs else \"\\\\hline\")\n",
    "    L.append(f\"Observations & {foot.loc['Observations','(1) Any offer']} & {foot.loc['Observations','(2) Individual/Joint']} \\\\\\\\\")\n",
    "    L.append(f\"R-squared & {foot.loc['R-squared','(1) Any offer']:.3f} & {foot.loc['R-squared','(2) Individual/Joint']:.3f} \\\\\\\\\")\n",
    "    L.append(\"\\\\bottomrule\" if use_booktabs else \"\\\\hline\")\n",
    "    L.append(\"\\\\end{tabular}\")\n",
    "    L.append(\"\\\\par\\\\vspace{0.5ex}\")\n",
    "    L.append(\"\\\\begin{minipage}{0.94\\\\linewidth}\\\\footnotesize\")\n",
    "    L.append(f\"Notes: Dependent variable {dep_label}. Cluster-robust SEs in parentheses (clustered by {cluster_name}). \")\n",
    "    L.append(f\"Controls are the LASSO-selected top {k_controls} baseline covariates from Appendix Figure~\\\\ref{{fig:lasso-top20}}, rescaled to $[0,1]$. \")\n",
    "    L.append(\"Province/stratum indicators are omitted in this sparse specification by design. \")\n",
    "    L.append(\"*, **, *** denote $p<0.05, p<0.01, p<0.001$.\")\n",
    "    L.append(\"\\\\end{minipage}\")\n",
    "    L.append(\"\\\\end{table}\")\n",
    "    return \"\\n\".join(L)\n",
    "\n",
    "dep_label = f\"$\\\\log(1+{Y_col})$\"\n",
    "cluster_name = cluster_col if cluster_col else \"village\"\n",
    "latex_str = to_latex_min(report, footer, dep_label, cluster_name, len(postlasso_controls))\n",
    "\n",
    "with open(\"tables/ancova_postlasso.tex\",\"w\") as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "# 9) Save audit trail of which controls were used\n",
    "pd.Series(postlasso_controls, name=\"postLASSO_controls\").to_csv(\"tables/postlasso_controls.csv\", index=False)\n",
    "\n",
    "print(\"[write] tables/ancova_postlasso.tex, tables/postlasso_controls.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d912f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Post-LASSO ANCOVA with province/stratum FE + \"hot\" baselines\n",
    "# y1 ~ y0 + Treatment + (Top-20 LASSO) + FE(stratum) + Always-keep\n",
    "# ============================================================\n",
    "import os, re, difflib\n",
    "import numpy as np, pandas as pd, statsmodels.api as sm\n",
    "\n",
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "\n",
    "# 0) Align design rows to the analysis sample\n",
    "X_unscaled = X_proc.loc[Y_use.index].copy()   # your imputed, one-hot baseline matrix\n",
    "\n",
    "# 1) LASSO top-20 (from your figure legend)\n",
    "LASSO_TOP20_RAW = [\n",
    "    \"redmeat2.x\",\"butter_ps0\",\"transer7.x\",\"transer8.x\",\"vehicl11.x\",\n",
    "    \"schoo12.x\",\"cladul12.x\",\"schoo11.x\",\"othecom7.x\",\"butter1.x\",\n",
    "    \"baselineY\",\"milk2.x\",\"othecom8.x\",\"flour2.x\",\"vegetab1.x\",\"rice1.x\",\n",
    "    \"othdair2.x\",\"choco3.x\",\"othdair1.x\",\"recreat7.x\"\n",
    "]\n",
    "\n",
    "# 2) Build: LASSO-selected controls (minus baselineY), add FE + Always-keep\n",
    "ALL_COLS = X_unscaled.columns.tolist()\n",
    "want = [v for v in LASSO_TOP20_RAW if v != \"baselineY\"]\n",
    "\n",
    "# exact matches\n",
    "present = [v for v in want if v in ALL_COLS]\n",
    "missing = [v for v in want if v not in ALL_COLS]\n",
    "\n",
    "# gentle fuzzy matches for near-misses (typos, suffixes)\n",
    "matched = {}\n",
    "for v in missing:\n",
    "    cand = difflib.get_close_matches(v, ALL_COLS, n=1, cutoff=0.8)\n",
    "    if cand:\n",
    "        matched[v] = cand[0]\n",
    "        present.append(cand[0])\n",
    "\n",
    "if matched:\n",
    "    print(\"[postLASSO] Fuzzy-matched:\", matched)\n",
    "if missing:\n",
    "    still_missing = [v for v in missing if v not in matched]\n",
    "    if still_missing:\n",
    "        print(f\"[postLASSO] WARNING not found: {still_missing}\")\n",
    "\n",
    "lasso_controls = set(present)\n",
    "\n",
    "# province/stratum fixed effects already in your design? grab any aimag/soum/region/province dummies\n",
    "FE_KEYS = (\"aimag\", \"province\", \"soum\", \"region\")\n",
    "fe_cols = [c for c in ALL_COLS if any(k in c.lower() for k in FE_KEYS)]\n",
    "\n",
    "# Optional: keep only dummy-like FE columns (few unique values)\n",
    "fe_cols = [c for c in fe_cols if X_unscaled[c].nunique(dropna=True) <= 50]\n",
    "print(f\"[postLASSO] Added FE columns: {len(fe_cols)}\")\n",
    "\n",
    "# \"Always-keep\" hot baseline controls (if present in your matrix)\n",
    "ALWAYS_KEEP_SEEDS = [\n",
    "    \"age\",\"age_head\",\"hhsize\",\"children\",\"dependents\",\"urban\",\"married\",\"educ_head\",\n",
    "    \"enterprise\",\"hours_ent\",\"hours_wage\",\"assets\",\"profit\"\n",
    "]\n",
    "def find_seed_cols(seeds, cols):\n",
    "    picks = []\n",
    "    for c in cols:\n",
    "        name = c.lower()\n",
    "        if any(re.search(rf\"\\b{re.escape(s)}(\\b|[_.=])\", name) for s in seeds):\n",
    "            picks.append(c)\n",
    "    return picks\n",
    "\n",
    "hot_cols = find_seed_cols(ALWAYS_KEEP_SEEDS, ALL_COLS)\n",
    "print(f\"[postLASSO] Added 'hot' baseline controls: {len(hot_cols)}\")\n",
    "\n",
    "# final selected baseline controls\n",
    "selected_controls = sorted(set(list(lasso_controls) + fe_cols + hot_cols))\n",
    "print(f\"[postLASSO] Total selected baseline controls: {len(selected_controls)}\")\n",
    "\n",
    "# 3) Column sets for pooled any-offer and two-arm specs\n",
    "cols_any = [\"baselineY\",\"treat_any\"] + selected_controls\n",
    "cols_two = [\"baselineY\",\"treat_indiv\",\"treat_group\"] + selected_controls\n",
    "\n",
    "# 4) Ensure numeric design\n",
    "def make_numeric_design(df, cols):\n",
    "    X = df[cols].copy()\n",
    "    if X.columns.duplicated().any():\n",
    "        X = X.loc[:, ~X.columns.duplicated()]\n",
    "    for c in X.columns:\n",
    "        if X[c].dtype == bool:\n",
    "            X[c] = X[c].astype(\"int8\")\n",
    "        elif str(X[c].dtype) == \"category\":\n",
    "            X[c] = X[c].cat.codes.replace(-1, np.nan)\n",
    "        elif X[c].dtype == \"O\":\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    for c in X.columns:\n",
    "        if X[c].isna().any():\n",
    "            X[c] = X[c].fillna(X[c].mean())\n",
    "    return X.astype(\"float64\")\n",
    "\n",
    "X_any = make_numeric_design(X_unscaled, cols_any)\n",
    "X_two = make_numeric_design(X_unscaled, cols_two)\n",
    "\n",
    "# 5) Cluster ids (village-level if available)\n",
    "cluster_candidates = [\"village_id\",\"cluster_id\",\"community_id\",\"village\",\"cluster\",\"soum\",\"aimag\"]\n",
    "cluster_col = next((c for c in cluster_candidates if c in Merged.columns), None)\n",
    "clusters = (Merged.loc[Y_use.index, cluster_col]\n",
    "            if cluster_col else pd.Series(np.arange(len(Y_use)), index=Y_use.index))\n",
    "\n",
    "# 6) Fit OLS with village-clustered SEs\n",
    "def fit_clustered(y, X, clusters):\n",
    "    y = np.asarray(y, dtype=\"float64\")\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    model = sm.OLS(y, X, missing=\"drop\")\n",
    "    return model.fit(cov_type=\"cluster\", cov_kwds={\"groups\": clusters})\n",
    "\n",
    "res_any = fit_clustered(Y_use, X_any, clusters)\n",
    "res_two = fit_clustered(Y_use, X_two, clusters)\n",
    "\n",
    "print(\"\\n=== Post-LASSO+FE ANCOVA (Any offer) ===\")\n",
    "print(res_any.summary())\n",
    "print(\"\\n=== Post-LASSO+FE ANCOVA (Two arms) ===\")\n",
    "print(res_two.summary())\n",
    "\n",
    "# 7) Compact report (Treatment(s), BaselineY, Constant only)\n",
    "def stars(p): return \"\" if p>=0.1 else \"*\" if p>=0.05 else \"**\" if p>=0.01 else \"***\"\n",
    "def pick_rows(res, wanted):\n",
    "    b, se, p = res.params, res.bse, res.pvalues\n",
    "    out = []\n",
    "    for lab, key in wanted:\n",
    "        if key is None:\n",
    "            val = b.get(\"const\", np.nan); s = se.get(\"const\", np.nan); pv = p.get(\"const\", 1.0)\n",
    "        else:\n",
    "            val = b[key]; s = se[key]; pv = p[key]\n",
    "        out.append((lab, f\"{val:0.3f}{stars(pv)}\\n({s:0.3f})\"))\n",
    "    return out\n",
    "\n",
    "rows_any = [(\"Treatment (Any offer)\",\"treat_any\"),\n",
    "            (\"Baseline outcome\",\"baselineY\"),\n",
    "            (\"Constant\",None)]\n",
    "rows_two = [(\"Treatment: Individual\",\"treat_indiv\"),\n",
    "            (\"Treatment: Joint\",\"treat_group\"),\n",
    "            (\"Baseline outcome\",\"baselineY\"),\n",
    "            (\"Constant\",None)]\n",
    "\n",
    "col_any = pick_rows(res_any, rows_any)\n",
    "col_two = pick_rows(res_two, rows_two)\n",
    "\n",
    "index = [r[0] for r in rows_two]\n",
    "report = pd.DataFrame(index=index, columns=[\"(1) Any offer\",\"(2) Individual/Joint\"])\n",
    "for lab, val in col_any: report.loc[lab, \"(1) Any offer\"] = val\n",
    "for lab, val in col_two: report.loc[lab, \"(2) Individual/Joint\"] = val\n",
    "\n",
    "footer = pd.DataFrame({\n",
    "    \"(1) Any offer\":[int(res_any.nobs), res_any.rsquared],\n",
    "    \"(2) Individual/Joint\":[int(res_two.nobs), res_two.rsquared],\n",
    "}, index=[\"Observations\",\"R-squared\"])\n",
    "\n",
    "# 8) LaTeX table (note now says FE + hot controls included)\n",
    "use_booktabs = True\n",
    "def to_latex_min(tbl, foot, dep_label, cluster_name, k_lasso, k_fe, k_hot):\n",
    "    spec = \"l\" + \"c\"*tbl.shape[1]\n",
    "    L = []\n",
    "    L.append(\"\\\\begin{table}[!htbp]\\\\centering\")\n",
    "    L.append(\"\\\\caption{Post-LASSO ANCOVA with Province/Stratum Fixed Effects and Hot Baseline Controls}\")\n",
    "    L.append(\"\\\\label{tab:ancova_postlasso}\")\n",
    "    L.append(\"\\\\begin{tabular}{\"+spec+\"}\")\n",
    "    L.append(\"\\\\toprule\" if use_booktabs else \"\\\\hline\")\n",
    "    L.append(\" & \" + \" & \".join(tbl.columns) + \" \\\\\\\\\")\n",
    "    L.append(\"\\\\midrule\" if use_booktabs else \"\\\\hline\")\n",
    "    for r in tbl.index:\n",
    "        L.append(f\"{r} & {tbl.loc[r, tbl.columns[0]] or ''} & {tbl.loc[r, tbl.columns[1]] or ''} \\\\\\\\\")\n",
    "    L.append(\"\\\\midrule\" if use_booktabs else \"\\\\hline\")\n",
    "    L.append(f\"Observations & {foot.loc['Observations','(1) Any offer']} & {foot.loc['Observations','(2) Individual/Joint']} \\\\\\\\\")\n",
    "    L.append(f\"R-squared & {foot.loc['R-squared','(1) Any offer']:.3f} & {foot.loc['R-squared','(2) Individual/Joint']:.3f} \\\\\\\\\")\n",
    "    L.append(\"\\\\bottomrule\" if use_booktabs else \"\\\\hline\")\n",
    "    L.append(\"\\\\end{tabular}\")\n",
    "    L.append(\"\\\\par\\\\vspace{0.5ex}\")\n",
    "    L.append(\"\\\\begin{minipage}{0.96\\\\linewidth}\\\\footnotesize\")\n",
    "    L.append(f\"Notes: Dependent variable {dep_label}. Cluster-robust SEs in parentheses (clustered by {cluster_name}). \")\n",
    "    L.append(f\"Controls include: LASSO-selected top {k_lasso} covariates (Appendix Figure~\\\\ref{{fig:lasso-top20}}), \"\n",
    "             f\"province/stratum fixed effects (\\\\(K={k_fe}\\\\)), and a small always-keep set of baseline demographics/enterprise proxies (\\\\(K={k_hot}\\\\)). \")\n",
    "    L.append(\"All covariates are from the pre-treatment baseline; treatment coefficients retain the ITT interpretation.\")\n",
    "    L.append(\"\\\\end{minipage}\")\n",
    "    L.append(\"\\\\end{table}\")\n",
    "    return \"\\n\".join(L)\n",
    "\n",
    "dep_label = f\"$\\\\log(1+{Y_col})$\"\n",
    "cluster_name = cluster_col if cluster_col else \"village\"\n",
    "latex_str = to_latex_min(\n",
    "    report, footer, dep_label, cluster_name,\n",
    "    k_lasso=len(lasso_controls), k_fe=len(fe_cols), k_hot=len(hot_cols)\n",
    ")\n",
    "\n",
    "with open(\"tables/ancova_postlasso.tex\",\"w\") as f:\n",
    "    f.write(latex_str)\n",
    "\n",
    "# 9) Save audit trail (tidy long form + optional one-row summary)\n",
    "\n",
    "# Long form: one row per variable with its type\n",
    "audit_rows = (\n",
    "    [{\"type\": \"lasso\", \"name\": c} for c in sorted(lasso_controls)] +\n",
    "    [{\"type\": \"fe\",    \"name\": c} for c in sorted(fe_cols)] +\n",
    "    [{\"type\": \"hot\",   \"name\": c} for c in sorted(set(hot_cols))]\n",
    ")\n",
    "audit_df = pd.DataFrame(audit_rows)\n",
    "audit_df.to_csv(\"tables/postlasso_controls_audit.csv\", index=False)\n",
    "\n",
    "# Optional: compact one-row summary (handy for quick reading)\n",
    "summary_df = pd.DataFrame({\n",
    "    \"k_lasso\": [len(lasso_controls)],\n",
    "    \"k_fe\":    [len(fe_cols)],\n",
    "    \"k_hot\":   [len(set(hot_cols))],\n",
    "    \"lasso_cols\": [\"; \".join(sorted(lasso_controls))],\n",
    "    \"fe_cols\":    [\"; \".join(sorted(fe_cols))],\n",
    "    \"hot_cols\":   [\"; \".join(sorted(set(hot_cols)))]\n",
    "})\n",
    "summary_df.to_csv(\"tables/postlasso_controls_summary.csv\", index=False)\n",
    "\n",
    "print(\"[write] tables/postlasso_controls_audit.csv, tables/postlasso_controls_summary.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762550a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa053b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Heuristic search for endline take-up variables\n",
    "#    Look for words: loan, borrow, credit, micro, MFI, take, adopt, client, member, etc.\n",
    "takeup_pattern = re.compile(r\"(adopt|take|took|borrow|loan|credit|micro|mfi|client|member|join)\", re.I)\n",
    "\n",
    "# Columns that *look* like endline (avoid baseline .x/0/base)\n",
    "def looks_endline(name: str) -> bool:\n",
    "    name = name.lower()\n",
    "    return (name.endswith(\".y\") or\n",
    "            re.search(r\"(end|follow|post|1)\\b\", name)) and not (\n",
    "            name.endswith(\".x\") or re.search(r\"(0|base|baseline)\\b\", name))\n",
    "\n",
    "cand_all = [c for c in Merged.columns if takeup_pattern.search(str(c))]\n",
    "cand_end = [c for c in cand_all if looks_endline(str(c))]\n",
    "\n",
    "# 2) Keep indicator-like variables\n",
    "def is_indicator_like(s: pd.Series) -> bool:\n",
    "    vals = pd.to_numeric(s, errors=\"coerce\")\n",
    "    u = set(vals.dropna().unique())\n",
    "    return len(u) > 0 and u.issubset({0,1})\n",
    "\n",
    "cand_bin = [c for c in cand_end if is_indicator_like(Merged[c])]\n",
    "\n",
    "print(\"[adoption] candidates (binary, endline-like):\", cand_bin[:25])\n",
    "\n",
    "# 3) First-stage sanity check: adoption should be higher in treatment villages\n",
    "df = Merged.loc[Y_use.index].copy()  # align to analysis sample\n",
    "first_stage = []\n",
    "for c in cand_bin:\n",
    "    v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    m_c = v[treat_any.loc[df.index]==0].mean()\n",
    "    m_t = v[treat_any.loc[df.index]==1].mean()\n",
    "    n   = v.notna().sum()\n",
    "    first_stage.append((c, n, m_c, m_t, m_t-m_c))\n",
    "\n",
    "first_stage = pd.DataFrame(first_stage, columns=[\"var\",\"N\",\"ctrl_mean\",\"treat_mean\",\"diff\"])\n",
    "first_stage = first_stage.sort_values(\"diff\", ascending=False)\n",
    "print(first_stage.head(15).to_string(index=False))\n",
    "\n",
    "# 4) Pick your adoption D:\n",
    "#    The best candidate is: (a) clearly binary, (b) measured at endline,\n",
    "#    (c) has a sizable treat-control difference, and ideally (d) refers to the *partner MFI* specifically.\n",
    "#    Suppose the top row looks good:\n",
    "if not first_stage.empty:\n",
    "    D_var = first_stage.iloc[0][\"var\"]\n",
    "    print(f\"[adoption] Suggested D variable: {D_var!r}\")\n",
    "else:\n",
    "    D_var = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bc560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
